# PyTorch Derin Ã–ÄŸrenme NotlarÄ±

PyTorch Ã¶ÄŸrenirken tuttuÄŸum notlar ve kod Ã¶rnekleri. Temel tensor iÅŸlemlerinden production deployment'a kadar 50 ders.

## Durum

**Tamamlanan:** 12/50 ders (%24)  
**Son gÃ¼ncelleme:** 18 Ocak 2026

## Tamamlanan BÃ¶lÃ¼mler

### âœ… Faz 1: Tensors & Autograd (7 ders)

Tensor'larÄ±n bellekte nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±, GPU transfer optimizasyonunu ve autograd mekanizmasÄ±nÄ± Ã¶ÄŸrendim.

1. **Tensor Mechanics** - Storage, stride, contiguous memory
2. **GEMM & Broadcasting** - Matris Ã§arpÄ±mÄ± optimizasyonu, BLAS
3. **Advanced Indexing** - Masking, fancy indexing, view vs copy
4. **View & Reshape** - Bellek optimizasyonu teknikleri
5. **GPU Acceleration** - CUDA, pinned memory, streams
6. **Autograd Engine** - DAG yapÄ±sÄ±, gradient hesaplama
7. **Custom Autograd** - Kendi tÃ¼rev fonksiyonunu yazma

**Ã–ÄŸrendiklerim:**
- Stride mekanizmasÄ± sayesinde transpose iÅŸlemi zero-copy
- CPU-GPU transfer darboÄŸazÄ± nasÄ±l Ã¶nlenir
- Manuel gradient vs autograd karÅŸÄ±laÅŸtÄ±rmasÄ±
- Custom activation function nasÄ±l yazÄ±lÄ±r

### âœ… Faz 2: Neural Networks (5 ders)

nn.Module'den optimizer'lara kadar neural network temellerini iÅŸledim.

8. **Linear Regression** - SÄ±fÄ±rdan, matematik ile (nn.Module yasak!)
9. **nn.Module Architecture** - Parameter yÃ¶netimi, hooks, state_dict
10. **Activation Functions** - ReLU, GELU, Swish, vanishing gradient
11. **Loss Functions** - MSE, CrossEntropy, Focal Loss
12. **Optimizers** - SGD, Momentum, Adam, AdamW

**Ã–ÄŸrendiklerim:**
- Manuel gradient descent vs autograd
- nn.Module'Ã¼n iÃ§inde ne oluyor
- Hangi activation ne zaman kullanÄ±lÄ±r
- Adam vs AdamW farkÄ± (weight decay)

## Planlanan BÃ¶lÃ¼mler

### ðŸ”„ Faz 3: Data Engineering (3 ders)

Dataset ve DataLoader optimizasyonlarÄ±.

13. Custom Dataset - `__len__`, `__getitem__` optimizasyonu
14. DataLoader - num_workers, pin_memory, collate_fn
15. Transforms - On-the-fly augmentation pipeline

### ðŸ”„ Faz 4: Training Loop (5 ders)

Production-grade training loop yazma.

16. Training Loop - model.train() vs model.eval()
17. Validation - torch.no_grad() vs inference_mode()
18. Checkpointing - state_dict, resume training
19. Logging - TensorBoard, loss curves
20. Regularization - L1/L2, dropout, early stopping

### ðŸ”„ Faz 5: Computer Vision (5 ders)

CNN'ler ve modern mimariler.

21. Convolution - Kernel, stride, padding, receptive field
22. Pooling - MaxPool, AvgPool, GlobalAveragePool
23. Normalization - BatchNorm, LayerNorm
24. CNN Architectures - ResNet, skip connections
25. Transfer Learning - Pretrained models, fine-tuning

### ðŸ”„ Faz 6: NLP & Sequences (5 ders)

RNN'ler ve attention mekanizmasÄ±.

26. RNN - Recurrent networks, BPTT
27. LSTM/GRU - Forget gate, input gate, output gate
28. Embeddings - nn.Embedding, Word2Vec
29. Seq2Seq - Encoder-decoder architecture
30. Attention - Attention mechanism (manuel)

### ðŸ”„ Faz 7: Transformers (5 ders)

Modern NLP'nin temeli.

31. Self-Attention - Multi-head attention (sÄ±fÄ±rdan)
32. Positional Encoding - Sinusoidal encoding
33. Layer Norm - Add & Norm blocks
34. Transformer Encoder - Tam encoder bloÄŸu
35. Transformer Decoder - Masked attention, causal masking

### ðŸ”„ Faz 8: Generative AI (4 ders)

VAE ve GAN'lar.

36. Autoencoder - Latent space manipulation
37. VAE - Reparameterization trick, KL divergence
38. GAN - Generator vs discriminator
39. DCGAN - Deep convolutional GAN

### ðŸ”„ Faz 9: Deployment (6 ders)

Production'a alma.

40. Quantization - FP32 â†’ INT8 dÃ¶nÃ¼ÅŸÃ¼mÃ¼
41. Pruning - Model budama
42. TorchScript - JIT compiler, tracing
43. ONNX Export - Cross-platform deployment
44. API Serving - Flask/FastAPI ile serving
45. Docker - GPU container hazÄ±rlama

### ðŸ”„ Faz 10: Projects (5 ders)

GerÃ§ek projeler.

46. Style Transfer - Neural style transfer (VGG)
47. BERT Fine-tuning - HuggingFace ile sentiment analysis
48. Image Captioning - CNN + LSTM
49. Mini-GPT - Character-level language model
50. Final Review - Ã–zet ve ileri seviye yol haritasÄ±

## KullanÄ±m

```bash
git clone https://github.com/tekay19/LearningDEEP.git
cd LearningDEEP

# Faz 1
cd Faz_1_Tensors
python 01_tensor_mechanics.py

# Faz 2
cd ../Faz_2_Neural_Networks
python 08_linear_regression_math.py
```

Her ders Ã§alÄ±ÅŸtÄ±rÄ±labilir Python kodu. BazÄ± derslerin yanÄ±nda ders notlarÄ± da var.

## Gereksinimler

- Python 3.8+
- PyTorch 2.0+
- NumPy
- Matplotlib (gÃ¶rselleÅŸtirme iÃ§in)

GPU opsiyonel, tÃ¼m kodlar CPU'da da Ã§alÄ±ÅŸÄ±r.

## Ä°lerleme

```
Faz 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (7/7)
Faz 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (5/5)
Faz 3: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/3)
Faz 4: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/5)
Faz 5: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/5)
Faz 6: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/5)
Faz 7: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/5)
Faz 8: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/4)
Faz 9: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (0/6)
Faz 10: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  0% (0/5)

Toplam: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 24% (12/50)
```

## Notlar

- Her ders type-hinted ve documented
- Production-ready kod Ã¶rnekleri
- Manuel implementasyonlar (nn.Module kullanmadan)
- GPU optimizasyonlarÄ± dahil

## Lisans

EÄŸitim amaÃ§lÄ± kullanÄ±m serbest.
