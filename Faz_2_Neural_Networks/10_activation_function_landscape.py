"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 10: ACTIVATION FUNCTIONS - ReLU, GELU, Swish VE VANISHING GRADIENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: Activation function'larÄ±n matematiksel Ã¶zelliklerini anlamak.
Vanishing/exploding gradient problemini Ã§Ã¶zmek.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from typing import Callable


def plot_activation(func: Callable, name: str, x_range: tuple = (-5, 5)) -> None:
    """Activation function ve tÃ¼revini Ã§izer."""
    x = torch.linspace(x_range[0], x_range[1], 1000, requires_grad=True)
    y = func(x)
    
    # TÃ¼rev hesapla
    y.sum().backward()
    grad = x.grad
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Fonksiyon
    ax1.plot(x.detach().numpy(), y.detach().numpy(), linewidth=2)
    ax1.axhline(0, color='k', linestyle='--', alpha=0.3)
    ax1.axvline(0, color='k', linestyle='--', alpha=0.3)
    ax1.set_title(f'{name}')
    ax1.set_xlabel('x')
    ax1.set_ylabel('f(x)')
    ax1.grid(True, alpha=0.3)
    
    # TÃ¼rev
    ax2.plot(x.detach().numpy(), grad.numpy(), linewidth=2, color='red')
    ax2.axhline(0, color='k', linestyle='--', alpha=0.3)
    ax2.axvline(0, color='k', linestyle='--', alpha=0.3)
    ax2.set_title(f"{name} - TÃ¼rev")
    ax2.set_xlabel('x')
    ax2.set_ylabel("f'(x)")
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'/tmp/{name.lower().replace(" ", "_")}.png', dpi=150)
    print(f"ğŸ“Š {name} grafiÄŸi kaydedildi")


def demonstrate_sigmoid() -> None:
    """Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))"""
    print("\n" + "ğŸ¯ SIGMOID".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = torch.sigmoid(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}")
    print(f"\nâœ… Avantajlar: Smooth, (0,1) aralÄ±ÄŸÄ±")
    print(f"âŒ Dezavantajlar: Vanishing gradient (xâ†’Â±âˆ iÃ§in tÃ¼revâ†’0)")
    
    plot_activation(torch.sigmoid, "Sigmoid")


def demonstrate_tanh() -> None:
    """Tanh: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
    print("\n" + "ğŸ¯ TANH".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = torch.tanh(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}")
    print(f"\nâœ… Avantajlar: Zero-centered, sigmoid'den iyi")
    print(f"âŒ Dezavantajlar: Hala vanishing gradient var")
    
    plot_activation(torch.tanh, "Tanh")


def demonstrate_relu() -> None:
    """ReLU: f(x) = max(0, x)"""
    print("\n" + "ğŸ¯ RELU".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = F.relu(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {y.tolist()}")
    print(f"\nâœ… Avantajlar: HÄ±zlÄ±, vanishing gradient yok (x>0)")
    print(f"âŒ Dezavantajlar: Dying ReLU (x<0 iÃ§in gradient=0)")
    
    plot_activation(F.relu, "ReLU")


def demonstrate_leaky_relu() -> None:
    """Leaky ReLU: f(x) = max(0.01x, x)"""
    print("\n" + "ğŸ¯ LEAKY RELU".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = F.leaky_relu(x, negative_slope=0.01)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}")
    print(f"\nâœ… Avantajlar: Dying ReLU problemi yok")
    print(f"âŒ Dezavantajlar: Negatif slope hyperparameter")


def demonstrate_gelu() -> None:
    """GELU: Gaussian Error Linear Unit"""
    print("\n" + "ğŸ¯ GELU (Transformer'larda kullanÄ±lÄ±r)".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = F.gelu(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}")
    print(f"\nâœ… Avantajlar: Smooth, probabilistic, BERT/GPT'de kullanÄ±lÄ±r")
    print(f"ğŸ’¡ FormÃ¼l: x * Î¦(x) (Î¦: Gaussian CDF)")
    
    plot_activation(F.gelu, "GELU")


def demonstrate_swish() -> None:
    """Swish (SiLU): f(x) = x * sigmoid(x)"""
    print("\n" + "ğŸ¯ SWISH / SiLU".center(70, "â”"))
    
    x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
    y = F.silu(x)  # Swish = SiLU
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}")
    print(f"\nâœ… Avantajlar: Self-gated, smooth, EfficientNet'te kullanÄ±lÄ±r")
    print(f"ğŸ’¡ FormÃ¼l: x * Ïƒ(x)")
    
    plot_activation(F.silu, "Swish (SiLU)")


def demonstrate_vanishing_gradient() -> None:
    """Vanishing gradient problemini gÃ¶sterir."""
    print("\n" + "ğŸ¯ VANISHING GRADIENT PROBLEMÄ°".center(70, "â”"))
    
    # Derin sigmoid network
    class DeepSigmoid(nn.Module):
        def __init__(self, depth: int = 10):
            super().__init__()
            layers = []
            for _ in range(depth):
                layers.append(nn.Linear(10, 10))
                layers.append(nn.Sigmoid())
            self.network = nn.Sequential(*layers)
        
        def forward(self, x):
            return self.network(x)
    
    # Test
    model = DeepSigmoid(depth=10)
    x = torch.randn(1, 10, requires_grad=True)
    y = model(x)
    y.sum().backward()
    
    print(f"\nğŸ“Š 10 KatmanlÄ± Sigmoid Network:")
    print(f"   Input gradient norm: {x.grad.norm().item():.2e}")
    
    # Katman gradientlerini incele
    print(f"\n   Katman Gradientleri:")
    for i, (name, param) in enumerate(model.named_parameters()):
        if param.grad is not None and 'weight' in name:
            print(f"   Layer {i//2}: {param.grad.norm().item():.2e}")
    
    print(f"\nâŒ Ä°lk katmanlarÄ±n gradienti Ã§ok kÃ¼Ã§Ã¼k (vanishing)!")


def demonstrate_activation_comparison() -> None:
    """FarklÄ± activation'larÄ± karÅŸÄ±laÅŸtÄ±rÄ±r."""
    print("\n" + "ğŸ¯ ACTIVATION KARÅILAÅTIRMASI".center(70, "â”"))
    
    x = torch.linspace(-5, 5, 100)
    
    activations = {
        'Sigmoid': torch.sigmoid(x),
        'Tanh': torch.tanh(x),
        'ReLU': F.relu(x),
        'GELU': F.gelu(x),
        'Swish': F.silu(x)
    }
    
    plt.figure(figsize=(10, 6))
    for name, y in activations.items():
        plt.plot(x.numpy(), y.numpy(), label=name, linewidth=2)
    
    plt.axhline(0, color='k', linestyle='--', alpha=0.3)
    plt.axvline(0, color='k', linestyle='--', alpha=0.3)
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.title('Activation Functions Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('/tmp/activation_comparison.png', dpi=150)
    print(f"\nğŸ“Š KarÅŸÄ±laÅŸtÄ±rma grafiÄŸi kaydedildi")


def main() -> None:
    print("\n" + "="*70)
    print("ğŸš€ ACTIVATION FUNCTIONS")
    print("="*70)
    
    demonstrate_sigmoid()
    demonstrate_tanh()
    demonstrate_relu()
    demonstrate_leaky_relu()
    demonstrate_gelu()
    demonstrate_swish()
    demonstrate_vanishing_gradient()
    demonstrate_activation_comparison()
    
    print("\n" + "="*70)
    print("âœ… DERS 10 TAMAMLANDI!")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
