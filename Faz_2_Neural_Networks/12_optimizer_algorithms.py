"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 12: OPTIMIZER ALGORITHMS - SGD, Momentum, Adam, AdamW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: Optimizer algoritmalarÄ±nÄ±n matematiksel Ã¶zelliklerini anlamak.
SGD'den AdamW'ye geÃ§iÅŸ sÃ¼recini Ã¶ÄŸrenmek.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import torch.nn as nn
import torch.optim as optim


def demonstrate_sgd() -> None:
    """Stochastic Gradient Descent"""
    print("\n" + "ðŸŽ¯ SGD (Stochastic Gradient Descent)".center(70, "â”"))
    
    # Basit model
    model = nn.Linear(2, 1)
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    
    print(f"ðŸ’¡ Update rule: Î¸ = Î¸ - lr * âˆ‡Î¸")
    print(f"   Learning rate: 0.01")
    
    # Dummy forward-backward
    x = torch.randn(10, 2)
    y = torch.randn(10, 1)
    
    pred = model(x)
    loss = F.mse_loss(pred, y)
    
    optimizer.zero_grad()
    loss.backward()
    
    print(f"\nÃ–nceki weight: {model.weight.data[0, 0].item():.4f}")
    optimizer.step()
    print(f"Sonraki weight: {model.weight.data[0, 0].item():.4f}")


def demonstrate_momentum() -> None:
    """SGD with Momentum"""
    print("\n" + "ðŸŽ¯ MOMENTUM".center(70, "â”"))
    
    model = nn.Linear(2, 1)
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    
    print(f"ðŸ’¡ Update rule:")
    print(f"   v_t = Î²*v_(t-1) + âˆ‡Î¸")
    print(f"   Î¸ = Î¸ - lr * v_t")
    print(f"\n   Momentum: 0.9")
    print(f"   âœ… Avantaj: Oscillation azalÄ±r, hÄ±zlanÄ±r")


def demonstrate_rmsprop() -> None:
    """RMSProp - Adaptive learning rate"""
    print("\n" + "ðŸŽ¯ RMSPROP".center(70, "â”"))
    
    model = nn.Linear(2, 1)
    optimizer = optim.RMSprop(model.parameters(), lr=0.01)
    
    print(f"ðŸ’¡ Update rule:")
    print(f"   E[gÂ²]_t = Î²*E[gÂ²]_(t-1) + (1-Î²)*gÂ²")
    print(f"   Î¸ = Î¸ - lr * g / âˆš(E[gÂ²] + Îµ)")
    print(f"\n   âœ… Her parametreye farklÄ± learning rate")


def demonstrate_adam() -> None:
    """Adam - Adaptive Moment Estimation"""
    print("\n" + "ðŸŽ¯ ADAM (En popÃ¼ler!)".center(70, "â”"))
    
    model = nn.Linear(2, 1)
    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
    
    print(f"ðŸ’¡ Update rule:")
    print(f"   m_t = Î²1*m_(t-1) + (1-Î²1)*g      (1st moment)")
    print(f"   v_t = Î²2*v_(t-1) + (1-Î²2)*gÂ²     (2nd moment)")
    print(f"   mÌ‚ = m_t / (1-Î²1^t)               (bias correction)")
    print(f"   vÌ‚ = v_t / (1-Î²2^t)")
    print(f"   Î¸ = Î¸ - lr * mÌ‚ / (âˆšvÌ‚ + Îµ)")
    print(f"\n   Î²1=0.9, Î²2=0.999")
    print(f"   âœ… Momentum + RMSProp birleÅŸimi")


def demonstrate_adamw() -> None:
    """AdamW - Adam with decoupled weight decay"""
    print("\n" + "ðŸŽ¯ ADAMW (Transformer'larda standart)".center(70, "â”"))
    
    model = nn.Linear(2, 1)
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    print(f"ðŸ’¡ Update rule:")
    print(f"   Adam update + weight decay:")
    print(f"   Î¸ = Î¸ - lr * mÌ‚ / (âˆšvÌ‚ + Îµ) - lr * Î» * Î¸")
    print(f"\n   Weight decay: 0.01")
    print(f"   âœ… L2 regularization'dan daha iyi")
    print(f"   âœ… BERT, GPT, ViT'de kullanÄ±lÄ±r")


def demonstrate_optimizer_comparison() -> None:
    """Optimizer'larÄ± karÅŸÄ±laÅŸtÄ±rÄ±r"""
    print("\n" + "ðŸŽ¯ OPTIMIZER KARÅžILAÅžTIRMASI".center(70, "â”"))
    
    # Basit problem: y = 3x + 2
    X = torch.randn(100, 1) * 10
    y = 3 * X + 2 + torch.randn(100, 1) * 0.5
    
    optimizers_config = {
        'SGD': lambda p: optim.SGD(p, lr=0.01),
        'SGD+Momentum': lambda p: optim.SGD(p, lr=0.01, momentum=0.9),
        'RMSprop': lambda p: optim.RMSprop(p, lr=0.01),
        'Adam': lambda p: optim.Adam(p, lr=0.01),
        'AdamW': lambda p: optim.AdamW(p, lr=0.01, weight_decay=0.01)
    }
    
    results = {}
    
    for name, opt_fn in optimizers_config.items():
        model = nn.Linear(1, 1)
        optimizer = opt_fn(model.parameters())
        
        # 100 epoch training
        for epoch in range(100):
            optimizer.zero_grad()
            pred = model(X)
            loss = F.mse_loss(pred, y)
            loss.backward()
            optimizer.step()
        
        final_loss = loss.item()
        results[name] = final_loss
        
        print(f"{name:15} â†’ Final loss: {final_loss:.6f}")
    
    best = min(results, key=results.get)
    print(f"\nðŸ† En iyi: {best}")


def demonstrate_learning_rate_scheduling() -> None:
    """Learning rate scheduling"""
    print("\n" + "ðŸŽ¯ LEARNING RATE SCHEDULING".center(70, "â”"))
    
    model = nn.Linear(2, 1)
    optimizer = optim.Adam(model.parameters(), lr=0.1)
    
    # StepLR: Her 10 epoch'ta lr'yi 0.1 ile Ã§arp
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    
    print(f"StepLR: Her 10 epoch'ta lr Ã— 0.1")
    
    for epoch in range(30):
        # Dummy training
        optimizer.zero_grad()
        loss = torch.tensor(1.0, requires_grad=True)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  Epoch {epoch+1}: lr = {current_lr:.6f}")


def main() -> None:
    print("\n" + "="*70)
    print("ðŸš€ OPTIMIZER ALGORITHMS")
    print("="*70)
    
    demonstrate_sgd()
    demonstrate_momentum()
    demonstrate_rmsprop()
    demonstrate_adam()
    demonstrate_adamw()
    demonstrate_optimizer_comparison()
    demonstrate_learning_rate_scheduling()
    
    print("\n" + "="*70)
    print("âœ… DERS 12 TAMAMLANDI!")
    print("="*70 + "\n")
    
    print("ðŸŽ‰ FAZ 2 TAMAMLANDI!")
    print("Sonraki: Faz 3 - Data Engineering")


if __name__ == "__main__":
    main()
