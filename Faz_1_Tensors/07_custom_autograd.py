"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 07: CUSTOM AUTOGRAD - KEND Ä° TÃœREV FONKSÄ°YONUNU YAZMA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: torch.autograd.Function sÄ±nÄ±fÄ±nÄ± miras alarak Ã¶zel tÃ¼rev fonksiyonlarÄ± yazmak.
Forward ve backward pass'leri manuel olarak tanÄ±mlamak.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import torch.nn as nn
from torch.autograd import Function
from typing import Tuple, Any, Optional
import math


class CustomReLU(Function):
    """
    ReLU fonksiyonunun custom implementasyonu.
    
    Forward: f(x) = max(0, x)
    Backward: df/dx = 1 if x > 0 else 0
    """
    
    @staticmethod
    def forward(ctx: Any, input: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: ReLU hesaplama.
        
        Args:
            ctx: Context object (backward iÃ§in veri saklamak iÃ§in)
            input: GiriÅŸ tensÃ¶rÃ¼
            
        Returns:
            ReLU uygulanmÄ±ÅŸ tensor
        """
        # Backward iÃ§in input'u sakla
        ctx.save_for_backward(input)
        
        # ReLU: max(0, x)
        output = input.clamp(min=0)
        
        return output
    
    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> torch.Tensor:
        """
        Backward pass: ReLU tÃ¼revi.
        
        Args:
            ctx: Context object (forward'dan gelen veri)
            grad_output: Ãœstten gelen gradient (âˆ‚L/âˆ‚output)
            
        Returns:
            Input'a gÃ¶re gradient (âˆ‚L/âˆ‚input)
        """
        # Forward'dan kaydedilen input'u al
        input, = ctx.saved_tensors
        
        # ReLU tÃ¼revi: 1 if x > 0 else 0
        grad_input = grad_output.clone()
        grad_input[input <= 0] = 0
        
        return grad_input


def demonstrate_custom_relu() -> None:
    """
    Custom ReLU implementasyonunu test eder.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 1: CUSTOM RELU - Ä°LK Ã–RNEK".center(70, "â”"))
    
    # Custom ReLU kullanÄ±mÄ±
    print("ğŸ”¹ Custom ReLU KullanÄ±mÄ±")
    
    x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    
    # Custom ReLU uygula
    y = CustomReLU.apply(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {y.tolist()}")
    print(f"y.grad_fn: {y.grad_fn}\n")
    
    # Backward pass
    print("â”€"*70)
    print("ğŸ”¹ Backward Pass")
    
    loss = y.sum()
    loss.backward()
    
    print(f"x.grad: {x.grad.tolist()}")
    print(f"ğŸ’¡ Gradient: 1 for x>0, 0 for xâ‰¤0\n")
    
    # PyTorch ReLU ile karÅŸÄ±laÅŸtÄ±rma
    print("â”€"*70)
    print("ğŸ”¹ PyTorch ReLU ile KarÅŸÄ±laÅŸtÄ±rma")
    
    x_torch = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    y_torch = torch.relu(x_torch)
    y_torch.sum().backward()
    
    print(f"PyTorch ReLU gradient: {x_torch.grad.tolist()}")
    print(f"Custom ReLU gradient:  {x.grad.tolist()}")
    print(f"âœ… EÅŸleÅŸiyor!")


class CustomSigmoid(Function):
    """
    Sigmoid fonksiyonunun custom implementasyonu.
    
    Forward: Ïƒ(x) = 1 / (1 + e^(-x))
    Backward: dÏƒ/dx = Ïƒ(x) Ã— (1 - Ïƒ(x))
    """
    
    @staticmethod
    def forward(ctx: Any, input: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: Sigmoid hesaplama.
        """
        output = 1 / (1 + torch.exp(-input))
        
        # Backward iÃ§in output'u sakla (tÃ¼rev hesabÄ±nda lazÄ±m)
        ctx.save_for_backward(output)
        
        return output
    
    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> torch.Tensor:
        """
        Backward pass: Sigmoid tÃ¼revi.
        
        dÏƒ/dx = Ïƒ(x) Ã— (1 - Ïƒ(x))
        """
        output, = ctx.saved_tensors
        
        # Sigmoid tÃ¼revi
        grad_input = grad_output * output * (1 - output)
        
        return grad_input


def demonstrate_custom_sigmoid() -> None:
    """
    Custom Sigmoid implementasyonunu test eder.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 2: CUSTOM SIGMOID - TÃœREV OPTÄ°MÄ°ZASYONU".center(70, "â”"))
    
    print("ğŸ”¹ Custom Sigmoid")
    
    x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    y = CustomSigmoid.apply(x)
    
    print(f"Input:  {x.tolist()}")
    print(f"Output: {[f'{v:.4f}' for v in y.tolist()]}\n")
    
    # Backward
    loss = y.sum()
    loss.backward()
    
    print(f"x.grad: {[f'{v:.4f}' for v in x.grad.tolist()]}")
    
    # Manuel doÄŸrulama
    print(f"\nğŸ§® MANUEL DOÄRULAMA (x=0):")
    print(f"Ïƒ(0) = 1/(1+e^0) = 0.5")
    print(f"dÏƒ/dx = Ïƒ(0) Ã— (1-Ïƒ(0)) = 0.5 Ã— 0.5 = 0.25")
    print(f"PyTorch sonucu: {x.grad[2].item():.4f}")
    print(f"âœ… EÅŸleÅŸiyor!")


class CustomLinear(Function):
    """
    Linear layer'Ä±n custom implementasyonu.
    
    Forward: y = x @ W^T + b
    Backward: 
        âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y @ W
        âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y^T @ x
        âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y.sum(dim=0)
    """
    
    @staticmethod
    def forward(ctx: Any, input: torch.Tensor, weight: torch.Tensor, 
                bias: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass: Linear transformation.
        
        Args:
            input: (batch, in_features)
            weight: (out_features, in_features)
            bias: (out_features,) or None
        """
        # Backward iÃ§in kaydet
        ctx.save_for_backward(input, weight, bias)
        
        # y = x @ W^T + b
        output = input.mm(weight.t())
        
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        
        return output
    
    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, ...]:
        """
        Backward pass: Linear layer gradients.
        
        Returns:
            (grad_input, grad_weight, grad_bias)
        """
        input, weight, bias = ctx.saved_tensors
        
        grad_input = grad_weight = grad_bias = None
        
        # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y @ W
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        
        # âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y^T @ x
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        
        # âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y.sum(dim=0)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)
        
        return grad_input, grad_weight, grad_bias


def demonstrate_custom_linear() -> None:
    """
    Custom Linear layer'Ä± test eder.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 3: CUSTOM LINEAR LAYER - MATMUL GRADÄ°ENT".center(70, "â”"))
    
    print("ğŸ”¹ Custom Linear Layer")
    
    batch_size, in_features, out_features = 4, 3, 2
    
    x = torch.randn(batch_size, in_features, requires_grad=True)
    W = torch.randn(out_features, in_features, requires_grad=True)
    b = torch.randn(out_features, requires_grad=True)
    
    print(f"Input shape: {x.shape}")
    print(f"Weight shape: {W.shape}")
    print(f"Bias shape: {b.shape}\n")
    
    # Custom linear
    y_custom = CustomLinear.apply(x, W, b)
    print(f"Output shape: {y_custom.shape}\n")
    
    # Backward
    loss = y_custom.sum()
    loss.backward()
    
    print(f"x.grad shape: {x.grad.shape}")
    print(f"W.grad shape: {W.grad.shape}")
    print(f"b.grad shape: {b.grad.shape}\n")
    
    # PyTorch nn.Linear ile karÅŸÄ±laÅŸtÄ±rma
    print("â”€"*70)
    print("ğŸ”¹ PyTorch nn.Linear ile KarÅŸÄ±laÅŸtÄ±rma")
    
    x_torch = x.detach().clone().requires_grad_(True)
    
    linear = nn.Linear(in_features, out_features, bias=True)
    linear.weight.data = W.detach().clone()
    linear.bias.data = b.detach().clone()
    
    y_torch = linear(x_torch)
    y_torch.sum().backward()
    
    print(f"Gradient farkÄ± (x): {(x.grad - x_torch.grad).abs().max().item():.2e}")
    print(f"Gradient farkÄ± (W): {(W.grad - linear.weight.grad).abs().max().item():.2e}")
    print(f"Gradient farkÄ± (b): {(b.grad - linear.bias.grad).abs().max().item():.2e}")
    print(f"âœ… Neredeyse sÄ±fÄ±r!")


class CustomGELU(Function):
    """
    GELU (Gaussian Error Linear Unit) custom implementasyonu.
    
    GELU(x) â‰ˆ 0.5 Ã— x Ã— (1 + tanh(âˆš(2/Ï€) Ã— (x + 0.044715 Ã— xÂ³)))
    """
    
    @staticmethod
    def forward(ctx: Any, input: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: GELU approximation.
        """
        # GELU approximation
        c = math.sqrt(2.0 / math.pi)
        tanh_arg = c * (input + 0.044715 * input.pow(3))
        output = 0.5 * input * (1.0 + torch.tanh(tanh_arg))
        
        # Backward iÃ§in kaydet
        ctx.save_for_backward(input, tanh_arg)
        
        return output
    
    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> torch.Tensor:
        """
        Backward pass: GELU tÃ¼revi.
        """
        input, tanh_arg = ctx.saved_tensors
        
        c = math.sqrt(2.0 / math.pi)
        tanh_val = torch.tanh(tanh_arg)
        sech2 = 1 - tanh_val.pow(2)
        
        # GELU tÃ¼revi (chain rule)
        grad_tanh_arg = c * (1 + 3 * 0.044715 * input.pow(2))
        grad_input = 0.5 * (1 + tanh_val) + 0.5 * input * sech2 * grad_tanh_arg
        
        return grad_output * grad_input


def demonstrate_custom_gelu() -> None:
    """
    Custom GELU implementasyonunu test eder.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 4: CUSTOM GELU - KARMAÅIK TÃœREV".center(70, "â”"))
    
    print("ğŸ”¹ Custom GELU")
    
    x = torch.linspace(-3, 3, 7, requires_grad=True)
    y_custom = CustomGELU.apply(x)
    
    print(f"Input:  {[f'{v:.2f}' for v in x.tolist()]}")
    print(f"Output: {[f'{v:.4f}' for v in y_custom.tolist()]}\n")
    
    # Backward
    y_custom.sum().backward()
    
    # PyTorch GELU ile karÅŸÄ±laÅŸtÄ±rma
    print("â”€"*70)
    print("ğŸ”¹ PyTorch GELU ile KarÅŸÄ±laÅŸtÄ±rma")
    
    x_torch = x.detach().clone().requires_grad_(True)
    y_torch = torch.nn.functional.gelu(x_torch, approximate='tanh')
    y_torch.sum().backward()
    
    print(f"Output farkÄ±: {(y_custom - y_torch).abs().max().item():.2e}")
    print(f"Gradient farkÄ±: {(x.grad - x_torch.grad).abs().max().item():.2e}")
    print(f"âœ… Ã‡ok kÃ¼Ã§Ã¼k fark!")


class CustomBatchNorm(Function):
    """
    Batch Normalization'Ä±n custom implementasyonu.
    
    Forward: y = (x - Î¼) / âˆš(ÏƒÂ² + Îµ) Ã— Î³ + Î²
    """
    
    @staticmethod
    def forward(ctx: Any, input: torch.Tensor, gamma: torch.Tensor, 
                beta: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
        """
        Forward pass: Batch normalization.
        
        Args:
            input: (batch, features)
            gamma: (features,) - scale parameter
            beta: (features,) - shift parameter
            eps: Numerical stability
        """
        # Batch statistics
        mean = input.mean(dim=0)
        var = input.var(dim=0, unbiased=False)
        
        # Normalize
        x_normalized = (input - mean) / torch.sqrt(var + eps)
        
        # Scale and shift
        output = gamma * x_normalized + beta
        
        # Backward iÃ§in kaydet
        ctx.save_for_backward(input, gamma, mean, var, x_normalized)
        ctx.eps = eps
        
        return output
    
    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, ...]:
        """
        Backward pass: BatchNorm gradients.
        """
        input, gamma, mean, var, x_normalized = ctx.saved_tensors
        eps = ctx.eps
        
        batch_size = input.size(0)
        
        # âˆ‚L/âˆ‚Î³
        grad_gamma = (grad_output * x_normalized).sum(dim=0)
        
        # âˆ‚L/âˆ‚Î²
        grad_beta = grad_output.sum(dim=0)
        
        # âˆ‚L/âˆ‚x (karmaÅŸÄ±k!)
        grad_x_normalized = grad_output * gamma
        
        std = torch.sqrt(var + eps)
        
        grad_var = (grad_x_normalized * (input - mean) * -0.5 * (var + eps).pow(-1.5)).sum(dim=0)
        grad_mean = (grad_x_normalized * -1 / std).sum(dim=0) + grad_var * (-2 * (input - mean)).sum(dim=0) / batch_size
        
        grad_input = grad_x_normalized / std + grad_var * 2 * (input - mean) / batch_size + grad_mean / batch_size
        
        return grad_input, grad_gamma, grad_beta, None


def demonstrate_custom_batchnorm() -> None:
    """
    Custom BatchNorm implementasyonunu test eder.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 5: CUSTOM BATCHNORM - EN KARMAÅIK TÃœREV".center(70, "â”"))
    
    print("ğŸ”¹ Custom BatchNorm")
    
    batch_size, features = 4, 3
    
    x = torch.randn(batch_size, features, requires_grad=True)
    gamma = torch.ones(features, requires_grad=True)
    beta = torch.zeros(features, requires_grad=True)
    
    print(f"Input shape: {x.shape}")
    print(f"Gamma shape: {gamma.shape}")
    print(f"Beta shape: {beta.shape}\n")
    
    # Custom BatchNorm
    y_custom = CustomBatchNorm.apply(x, gamma, beta)
    
    print(f"Output mean: {y_custom.mean(dim=0).tolist()}")
    print(f"Output var: {y_custom.var(dim=0, unbiased=False).tolist()}")
    print(f"ğŸ’¡ Mean â‰ˆ 0, Var â‰ˆ 1 (normalization Ã§alÄ±ÅŸtÄ±!)\n")
    
    # Backward
    y_custom.sum().backward()
    
    print(f"x.grad shape: {x.grad.shape}")
    print(f"gamma.grad: {gamma.grad.tolist()}")
    print(f"beta.grad: {beta.grad.tolist()}")


def demonstrate_gradient_check() -> None:
    """
    Numerical gradient checking ile custom gradient'leri doÄŸrular.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 6: GRADIENT CHECKING - DOÄRULAMA".center(70, "â”"))
    
    print("ğŸ”¹ Numerical Gradient vs Analytical Gradient")
    
    from torch.autograd import gradcheck
    
    # CustomReLU test
    print("\nğŸ”¹ CustomReLU Gradient Check")
    
    x = torch.randn(5, dtype=torch.double, requires_grad=True)
    
    # gradcheck: numerical vs analytical gradient karÅŸÄ±laÅŸtÄ±rmasÄ±
    test_passed = gradcheck(CustomReLU.apply, x, eps=1e-6, atol=1e-4)
    
    print(f"Gradient check: {'âœ… PASSED' if test_passed else 'âŒ FAILED'}")
    
    # CustomSigmoid test
    print("\nğŸ”¹ CustomSigmoid Gradient Check")
    
    x = torch.randn(5, dtype=torch.double, requires_grad=True)
    test_passed = gradcheck(CustomSigmoid.apply, x, eps=1e-6, atol=1e-4)
    
    print(f"Gradient check: {'âœ… PASSED' if test_passed else 'âŒ FAILED'}")
    
    print(f"\nğŸ’¡ gradcheck, numerical differentiation ile analytical gradient'i karÅŸÄ±laÅŸtÄ±rÄ±r")
    print(f"   Numerical: f'(x) â‰ˆ (f(x+Îµ) - f(x-Îµ)) / (2Îµ)")


def demonstrate_common_pitfalls() -> None:
    """
    Custom autograd yazarken sÄ±k yapÄ±lan hatalarÄ± gÃ¶sterir.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 7: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    print("ğŸ”´ HATA 1: ctx.save_for_backward() Unutmak")
    print("""
    # YANLIÅ
    @staticmethod
    def forward(ctx, input):
        output = input * 2
        return output  # input kaydedilmedi!
    
    @staticmethod
    def backward(ctx, grad_output):
        # input'a eriÅŸemeyiz! HATA!
        return grad_output * 2
    
    # DOÄRU
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)  # Kaydet!
        return input * 2
    """)
    
    print("\n" + "â”€"*70)
    print("ğŸ”´ HATA 2: Backward'da YanlÄ±ÅŸ SayÄ±da Gradient DÃ¶ndÃ¼rmek")
    print("""
    # Forward 3 parametre alÄ±yor
    def forward(ctx, input, weight, bias):
        ...
    
    # YANLIÅ: Backward 2 gradient dÃ¶ndÃ¼rÃ¼yor
    def backward(ctx, grad_output):
        return grad_input, grad_weight  # bias eksik!
    
    # DOÄRU: Her parametre iÃ§in gradient dÃ¶ndÃ¼r (None olabilir)
    def backward(ctx, grad_output):
        return grad_input, grad_weight, grad_bias
    """)
    
    print("\n" + "â”€"*70)
    print("ğŸ”´ HATA 3: In-place Ä°ÅŸlem Kullanmak")
    print("""
    # YANLIÅ
    @staticmethod
    def backward(ctx, grad_output):
        grad_output[grad_output < 0] = 0  # In-place!
        return grad_output
    
    # DOÄRU
    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()  # Kopya oluÅŸtur
        grad_input[grad_input < 0] = 0
        return grad_input
    """)


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ğŸš€ CUSTOM AUTOGRAD - KEND Ä° TÃœREV FONKSÄ°YONUNU YAZMA".center(70))
    print("="*70)
    
    demonstrate_custom_relu()
    demonstrate_custom_sigmoid()
    demonstrate_custom_linear()
    demonstrate_custom_gelu()
    demonstrate_custom_batchnorm()
    demonstrate_gradient_check()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 07 TAMAMLANDI!".center(70))
    print("="*70 + "\n")
    
    print("ğŸ‰ FAZ 1 TAMAMLANDI! ğŸ‰".center(70))
    print("Tensors & Computational Graph konularÄ±nÄ± bitirdiniz!".center(70))
    print("\nğŸš€ Sonraki: Faz 2 - Neural Network Fundamentals".center(70))


if __name__ == "__main__":
    main()
