"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 06: AUTOGRAD ENGINE - DAG, BACKWARD VE GRADIENT FLOW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: PyTorch'un otomatik tÃ¼rev mekanizmasÄ±nÄ± anlamak.
DAG (Directed Acyclic Graph) yapÄ±sÄ±nÄ± ve .backward() Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmek.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
from typing import Tuple, List, Optional
import graphviz  # pip install graphviz


def demonstrate_basic_autograd() -> None:
    """
    Temel autograd mekanizmasÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 1: TEMEL AUTOGRAD - OTOMATÄ°K TÃœREV".center(70, "â”"))
    
    # requires_grad=True ile tensor oluÅŸturma
    print("ðŸ”¹ Gradient Takibi Aktif Tensor")
    
    x = torch.tensor([2.0, 3.0], requires_grad=True)
    print(f"x = {x}")
    print(f"x.requires_grad = {x.requires_grad}")
    print(f"x.grad = {x.grad} (HenÃ¼z backward() Ã§aÄŸrÄ±lmadÄ±)\n")
    
    # Basit bir iÅŸlem
    print("â”€"*70)
    print("ðŸ”¹ Ä°ÅŸlem: y = xÂ² + 3x + 1")
    
    y = x**2 + 3*x + 1
    print(f"y = {y}")
    print(f"y.requires_grad = {y.requires_grad}")
    print(f"y.grad_fn = {y.grad_fn}")
    print(f"ðŸ’¡ grad_fn: Bu tensor'u oluÅŸturan iÅŸlem (AddBackward)\n")
    
    # Skaler Ã§Ä±ktÄ± iÃ§in backward
    print("â”€"*70)
    print("ðŸ”¹ Backward: Gradient Hesaplama")
    
    loss = y.sum()  # Skaler yapmalÄ±yÄ±z
    print(f"loss = y.sum() = {loss}")
    print(f"loss.grad_fn = {loss.grad_fn}\n")
    
    loss.backward()
    print(f"âœ… loss.backward() Ã§aÄŸrÄ±ldÄ±!")
    print(f"x.grad = {x.grad}")
    
    # Manuel doÄŸrulama
    print(f"\nðŸ§® MANUEL DOÄžRULAMA:")
    print(f"dy/dx = 2x + 3")
    print(f"x=2 iÃ§in: 2(2) + 3 = 7")
    print(f"x=3 iÃ§in: 2(3) + 3 = 9")
    print(f"PyTorch sonucu: {x.grad.tolist()}")
    print(f"âœ… EÅŸleÅŸiyor!")


def demonstrate_computational_graph() -> None:
    """
    Computational graph (hesaplama grafiÄŸi) yapÄ±sÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 2: COMPUTATIONAL GRAPH - DAG YAPISI".center(70, "â”"))
    
    # Daha karmaÅŸÄ±k bir graph
    print("ðŸ”¹ KarmaÅŸÄ±k Hesaplama GrafiÄŸi")
    
    a = torch.tensor([2.0], requires_grad=True)
    b = torch.tensor([3.0], requires_grad=True)
    
    print(f"a = {a.item()}, b = {b.item()}\n")
    
    # Ä°ÅŸlemler
    c = a * b           # c = 2 * 3 = 6
    d = a + b           # d = 2 + 3 = 5
    e = c * d           # e = 6 * 5 = 30
    f = e.relu()        # f = max(0, 30) = 30
    loss = f.sum()      # loss = 30
    
    print(f"c = a * b = {c.item()}")
    print(f"d = a + b = {d.item()}")
    print(f"e = c * d = {e.item()}")
    print(f"f = relu(e) = {f.item()}")
    print(f"loss = {loss.item()}\n")
    
    # Graph yapÄ±sÄ±nÄ± gÃ¶ster
    print("â”€"*70)
    print("ðŸ”¹ Graph YapÄ±sÄ± (grad_fn chain)")
    
    print(f"loss.grad_fn = {loss.grad_fn}")
    print(f"  â””â”€ f.grad_fn = {f.grad_fn}")
    print(f"      â””â”€ e.grad_fn = {e.grad_fn}")
    print(f"          â”œâ”€ c.grad_fn = {c.grad_fn}")
    print(f"          â””â”€ d.grad_fn = {d.grad_fn}\n")
    
    # Backward pass
    print("â”€"*70)
    print("ðŸ”¹ Backward Pass: Gradient AkÄ±ÅŸÄ±")
    
    loss.backward()
    
    print(f"âˆ‚loss/âˆ‚a = {a.grad.item()}")
    print(f"âˆ‚loss/âˆ‚b = {b.grad.item()}")
    
    # Manuel hesaplama
    print(f"\nðŸ§® MANUEL HESAPLAMA:")
    print(f"âˆ‚loss/âˆ‚e = âˆ‚f/âˆ‚e = 1 (relu tÃ¼revi, e>0 iÃ§in)")
    print(f"âˆ‚loss/âˆ‚c = âˆ‚loss/âˆ‚e Ã— âˆ‚e/âˆ‚c = 1 Ã— d = {d.item()}")
    print(f"âˆ‚loss/âˆ‚d = âˆ‚loss/âˆ‚e Ã— âˆ‚e/âˆ‚d = 1 Ã— c = {c.item()}")
    print(f"âˆ‚loss/âˆ‚a = âˆ‚loss/âˆ‚c Ã— âˆ‚c/âˆ‚a + âˆ‚loss/âˆ‚d Ã— âˆ‚d/âˆ‚a")
    print(f"         = {d.item()} Ã— {b.item()} + {c.item()} Ã— 1 = {d.item() * b.item() + c.item()}")
    print(f"âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚c Ã— âˆ‚c/âˆ‚b + âˆ‚loss/âˆ‚d Ã— âˆ‚d/âˆ‚b")
    print(f"         = {d.item()} Ã— {a.item()} + {c.item()} Ã— 1 = {d.item() * a.item() + c.item()}")


def demonstrate_gradient_accumulation() -> None:
    """
    Gradient accumulation (biriktirme) mekanizmasÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 3: GRADIENT ACCUMULATION - BÄ°RÄ°KTÄ°RME".center(70, "â”"))
    
    # Ä°lk backward
    print("ðŸ”¹ Ä°lk Backward")
    
    x = torch.tensor([2.0], requires_grad=True)
    
    y1 = x**2
    y1.backward()
    
    print(f"y1 = xÂ² = {y1.item()}")
    print(f"âˆ‚y1/âˆ‚x = 2x = {x.grad.item()}\n")
    
    # Ä°kinci backward (gradient birikiyor!)
    print("â”€"*70)
    print("ðŸ”´ Ä°kinci Backward (Gradient Birikiyor!)")
    
    y2 = x**3
    y2.backward()
    
    print(f"y2 = xÂ³ = {y2.item()}")
    print(f"x.grad = {x.grad.item()}")
    print(f"ðŸ’¡ Beklenen: âˆ‚y2/âˆ‚x = 3xÂ² = {3 * x.item()**2}")
    print(f"âš ï¸  Ama sonuÃ§: {x.grad.item()} (Ã–nceki gradient eklendi!)\n")
    
    # Gradient sÄ±fÄ±rlama
    print("â”€"*70)
    print("âœ… Gradient SÄ±fÄ±rlama")
    
    x.grad.zero_()  # veya x.grad = None
    
    y3 = x**3
    y3.backward()
    
    print(f"x.grad.zero_() Ã§aÄŸrÄ±ldÄ±")
    print(f"y3 = xÂ³ backward sonrasÄ±:")
    print(f"x.grad = {x.grad.item()}")
    print(f"âœ… DoÄŸru sonuÃ§: {3 * x.item()**2}\n")
    
    # Pratik: Training loop'ta kullanÄ±m
    print("â”€"*70)
    print("ðŸ”¹ PRATÄ°K: Training Loop'ta KullanÄ±m")
    
    print("""
    # YANLIÅž
    for epoch in range(10):
        loss = model(x)
        loss.backward()  # Gradientler birikiyor!
        optimizer.step()
    
    # DOÄžRU
    for epoch in range(10):
        optimizer.zero_grad()  # Gradientleri sÄ±fÄ±rla
        loss = model(x)
        loss.backward()
        optimizer.step()
    """)


def demonstrate_no_grad_context() -> None:
    """
    torch.no_grad() ve torch.inference_mode() kullanÄ±mÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 4: NO_GRAD VE INFERENCE_MODE".center(70, "â”"))
    
    # Normal mod (gradient tracking)
    print("ðŸ”¹ Normal Mod (Gradient Tracking)")
    
    x = torch.randn(1000, 1000, requires_grad=True)
    
    import time
    start = time.time()
    y = x @ x
    z = y.sum()
    normal_time = time.time() - start
    
    print(f"y.requires_grad = {y.requires_grad}")
    print(f"y.grad_fn = {y.grad_fn}")
    print(f"SÃ¼re: {normal_time*1000:.4f} ms\n")
    
    # torch.no_grad() context
    print("â”€"*70)
    print("ðŸ”¹ torch.no_grad() Context")
    
    with torch.no_grad():
        start = time.time()
        y = x @ x
        z = y.sum()
        no_grad_time = time.time() - start
        
        print(f"y.requires_grad = {y.requires_grad}")
        print(f"y.grad_fn = {y.grad_fn}")
        print(f"SÃ¼re: {no_grad_time*1000:.4f} ms")
        print(f"ðŸš€ {normal_time/no_grad_time:.2f}x daha hÄ±zlÄ±!\n")
    
    # torch.inference_mode() (PyTorch 1.9+)
    print("â”€"*70)
    print("ðŸ”¹ torch.inference_mode() (Daha HÄ±zlÄ±)")
    
    with torch.inference_mode():
        start = time.time()
        y = x @ x
        z = y.sum()
        inference_time = time.time() - start
        
        print(f"y.requires_grad = {y.requires_grad}")
        print(f"SÃ¼re: {inference_time*1000:.4f} ms")
        print(f"ðŸš€ {normal_time/inference_time:.2f}x daha hÄ±zlÄ±!")
        print(f"ðŸ’¡ inference_mode, no_grad'dan daha agresif optimizasyon yapar\n")
    
    # Decorator kullanÄ±mÄ±
    print("â”€"*70)
    print("ðŸ”¹ Decorator KullanÄ±mÄ±")
    
    print("""
    @torch.no_grad()
    def evaluate(model, data):
        predictions = model(data)
        return predictions
    
    @torch.inference_mode()
    def predict(model, data):
        return model(data)  # Daha hÄ±zlÄ±!
    """)


def demonstrate_retain_graph() -> None:
    """
    retain_graph parametresini aÃ§Ä±klar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 5: RETAIN_GRAPH - GRAPH'I KORUMA".center(70, "â”"))
    
    x = torch.tensor([2.0], requires_grad=True)
    
    y = x**2
    z = y * 3
    
    print(f"y = xÂ² = {y.item()}")
    print(f"z = y Ã— 3 = {z.item()}\n")
    
    # Ä°lk backward
    print("â”€"*70)
    print("ðŸ”¹ Ä°lk Backward (y)")
    
    y.backward(retain_graph=True)
    print(f"y.backward(retain_graph=True)")
    print(f"x.grad = {x.grad.item()}\n")
    
    # Ä°kinci backward (aynÄ± graph)
    print("â”€"*70)
    print("ðŸ”¹ Ä°kinci Backward (z)")
    
    x.grad.zero_()
    z.backward()
    print(f"z.backward()")
    print(f"x.grad = {x.grad.item()}")
    
    # retain_graph=False durumu
    print("\n" + "â”€"*70)
    print("ðŸ”´ retain_graph=False (Default)")
    
    x = torch.tensor([2.0], requires_grad=True)
    y = x**2
    
    y.backward()  # Graph silinir
    
    try:
        y.backward()  # HATA: Graph yok!
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"ðŸ’¡ Graph bir kez kullanÄ±ldÄ±ktan sonra silinir (bellek tasarrufu)")


def demonstrate_higher_order_gradients() -> None:
    """
    Ä°kinci dereceden tÃ¼revleri (Hessian) hesaplar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 6: HIGHER-ORDER GRADIENTS - Ä°KÄ°NCÄ° TÃœREV".center(70, "â”"))
    
    # Birinci tÃ¼rev
    print("ðŸ”¹ Birinci TÃ¼rev")
    
    x = torch.tensor([2.0], requires_grad=True)
    y = x**3  # y = xÂ³
    
    print(f"y = xÂ³ = {y.item()}")
    
    # dy/dx
    grad_y = torch.autograd.grad(y, x, create_graph=True)[0]
    print(f"dy/dx = 3xÂ² = {grad_y.item()}\n")
    
    # Ä°kinci tÃ¼rev
    print("â”€"*70)
    print("ðŸ”¹ Ä°kinci TÃ¼rev (Hessian)")
    
    # dÂ²y/dxÂ²
    grad2_y = torch.autograd.grad(grad_y, x)[0]
    print(f"dÂ²y/dxÂ² = 6x = {grad2_y.item()}")
    print(f"Manuel: 6 Ã— {x.item()} = {6 * x.item()}")
    print(f"âœ… EÅŸleÅŸiyor!\n")
    
    # Pratik: Newton's Method
    print("â”€"*70)
    print("ðŸ”¹ PRATÄ°K: Newton's Method Optimizasyonu")
    
    print("""
    # Newton's Method: x_new = x - f'(x) / f''(x)
    
    x = torch.tensor([1.0], requires_grad=True)
    
    for i in range(10):
        y = (x - 2)**2  # Minimize edilecek fonksiyon
        
        grad1 = torch.autograd.grad(y, x, create_graph=True)[0]
        grad2 = torch.autograd.grad(grad1, x)[0]
        
        x.data -= grad1 / grad2  # Newton update
    
    # x â†’ 2.0'a yakÄ±nsar (minimum nokta)
    """)


def demonstrate_gradient_checkpointing() -> None:
    """
    Gradient checkpointing ile bellek optimizasyonu.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 7: GRADIENT CHECKPOINTING - BELLEK OPTÄ°MÄ°ZASYONU".center(70, "â”"))
    
    print("ðŸ”¹ Normal Backward (TÃ¼m Intermediate DeÄŸerler SaklanÄ±r)")
    
    print("""
    # Normal backward
    x = torch.randn(1000, 1000, requires_grad=True)
    
    y1 = x @ x
    y2 = y1 @ y1
    y3 = y2 @ y2
    loss = y3.sum()
    
    loss.backward()
    
    # Bellek: y1, y2, y3 hepsi saklanÄ±r (backward iÃ§in gerekli)
    # Toplam: ~4 GB (bÃ¼yÃ¼k modellerde problem!)
    """)
    
    print("\n" + "â”€"*70)
    print("ðŸ”¹ Gradient Checkpointing (Bellek Tasarrufu)")
    
    print("""
    from torch.utils.checkpoint import checkpoint
    
    def compute_block(x):
        y1 = x @ x
        y2 = y1 @ y1
        return y2
    
    x = torch.randn(1000, 1000, requires_grad=True)
    
    # Checkpointing kullan
    y = checkpoint(compute_block, x)
    loss = y.sum()
    loss.backward()
    
    # Bellek: Sadece checkpoint noktalarÄ± saklanÄ±r
    # Backward sÄ±rasÄ±nda intermediate deÄŸerler yeniden hesaplanÄ±r
    # Trade-off: %50 bellek tasarrufu, %30 yavaÅŸlama
    """)
    
    print(f"\nðŸ’¡ KullanÄ±m AlanÄ±: Transformer'lar, Ã§ok derin CNN'ler")


def demonstrate_common_pitfalls() -> None:
    """
    Autograd kullanÄ±mÄ±nda sÄ±k yapÄ±lan hatalarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 8: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    # HATA 1: In-place iÅŸlem
    print("ðŸ”´ HATA 1: In-place Ä°ÅŸlem Gradient Graph'Ä± Bozar")
    
    x = torch.tensor([2.0], requires_grad=True)
    y = x**2
    
    try:
        # YANLIÅž: In-place iÅŸlem
        y.add_(1.0)  # y += 1
        y.backward()
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"ðŸ’¡ In-place iÅŸlemler (_ile bitenler) gradient graph'Ä± bozar\n")
    
    # HATA 2: Leaf variable'a in-place iÅŸlem
    print("â”€"*70)
    print("ðŸ”´ HATA 2: Leaf Variable'a In-place Ä°ÅŸlem")
    
    x = torch.tensor([2.0], requires_grad=True)
    
    try:
        x.add_(1.0)  # HATA!
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"ðŸ’¡ Leaf variable'lar (input) deÄŸiÅŸtirilemez\n")
    
    # HATA 3: Non-scalar backward
    print("â”€"*70)
    print("ðŸ”´ HATA 3: Non-scalar Tensor'da backward()")
    
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    y = x**2
    
    try:
        y.backward()  # HATA: y skaler deÄŸil!
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"ðŸ’¡ Ã‡Ã–ZÃœM 1: .sum() ile skaler yap")
        
        x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
        y = x**2
        y.sum().backward()
        print(f"y.sum().backward() â†’ x.grad = {x.grad}")
        
        print(f"\nðŸ’¡ Ã‡Ã–ZÃœM 2: gradient parametresi ver")
        x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
        y = x**2
        y.backward(torch.ones_like(y))
        print(f"y.backward(torch.ones_like(y)) â†’ x.grad = {x.grad}")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ðŸš€ AUTOGRAD ENGINE - OTOMATÄ°K TÃœREV MEKANÄ°ZMASI".center(70))
    print("="*70)
    
    demonstrate_basic_autograd()
    demonstrate_computational_graph()
    demonstrate_gradient_accumulation()
    demonstrate_no_grad_context()
    demonstrate_retain_graph()
    demonstrate_higher_order_gradients()
    demonstrate_gradient_checkpointing()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 06 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
