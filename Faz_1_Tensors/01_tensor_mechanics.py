"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 01: TENSOR MEKANÄ°ÄÄ° - BELLEK DÃœZENÄ° VE STRIDE ANALÄ°ZÄ°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: PyTorch Tensor'larÄ±nÄ±n NumPy dizilerinden farkÄ±nÄ± anlamak.
Storage, Offset ve Stride kavramlarÄ±nÄ± bellek dÃ¼zeyinde incelemek.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
from typing import Tuple, Any
import sys


def inspect_tensor_anatomy(tensor: torch.Tensor, name: str = "Tensor") -> None:
    """
    Bir tensÃ¶rÃ¼n tÃ¼m anatomik Ã¶zelliklerini detaylÄ± ÅŸekilde yazdÄ±rÄ±r.
    
    Args:
        tensor: Ä°ncelenecek PyTorch tensÃ¶rÃ¼
        name: TensÃ¶rÃ¼n tanÄ±mlayÄ±cÄ± ismi
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ {name} ANATOMÄ°K ANALÄ°Z")
    print(f"{'='*70}")
    print(f"ğŸ“Š Shape (Boyut):        {tensor.shape}")
    print(f"ğŸ§® Dtype (Veri Tipi):    {tensor.dtype}")
    print(f"ğŸ“ Stride (AdÄ±m):        {tensor.stride()}")
    print(f"ğŸ’¾ Storage Size:         {tensor.storage().size()} elements")
    print(f"ğŸ“ Storage Offset:       {tensor.storage_offset()}")
    print(f"ğŸ–¥ï¸  Device (Cihaz):       {tensor.device}")
    print(f"ğŸ“ Requires Grad:        {tensor.requires_grad}")
    print(f"ğŸ”— Is Contiguous:        {tensor.is_contiguous()}")
    print(f"ğŸ’½ Memory (bytes):       {tensor.element_size() * tensor.nelement()}")
    print(f"{'='*70}\n")


def demonstrate_tensor_vs_numpy() -> None:
    """
    PyTorch Tensor ile NumPy Array arasÄ±ndaki temel farklarÄ± gÃ¶sterir.
    Ã–zellikle GPU desteÄŸi ve autograd Ã¶zelliklerini vurgular.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 1: TENSOR VS NUMPY - TEMEL FARKLAR".center(70, "â”"))
    
    # NumPy array oluÅŸturma
    np_array = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)
    print(f"\nğŸ“¦ NumPy Array:\n{np_array}")
    print(f"Type: {type(np_array)}, Dtype: {np_array.dtype}")
    
    # PyTorch tensor oluÅŸturma (NumPy'dan)
    tensor_from_numpy = torch.from_numpy(np_array)
    inspect_tensor_anatomy(tensor_from_numpy, "NumPy'dan DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ Tensor")
    
    # âš ï¸ KRÄ°TÄ°K: NumPy ve Tensor aynÄ± belleÄŸi paylaÅŸÄ±r!
    print("ğŸ”´ BELLEK PAYLAÅIMI TESTÄ°:")
    np_array[0, 0] = 999
    print(f"NumPy deÄŸiÅŸtirildi -> np_array[0,0] = {np_array[0, 0]}")
    print(f"Tensor otomatik gÃ¼ncellendi -> tensor[0,0] = {tensor_from_numpy[0, 0]}")
    print("âš¡ SonuÃ§: AynÄ± bellek bÃ¶lgesini gÃ¶steriyorlar (Zero-copy operation)\n")
    
    # SÄ±fÄ±rdan PyTorch tensor oluÅŸturma
    pure_tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], 
                                dtype=torch.float32, 
                                requires_grad=True)  # Gradient takibi aktif
    inspect_tensor_anatomy(pure_tensor, "Saf PyTorch Tensor (Gradient Aktif)")


def demonstrate_storage_and_offset() -> None:
    """
    PyTorch'un Storage mekanizmasÄ±nÄ± ve Offset kavramÄ±nÄ± aÃ§Ä±klar.
    Birden fazla tensor'un aynÄ± storage'Ä± nasÄ±l paylaÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 2: STORAGE VE OFFSET - BELLEK OPTÄ°MÄ°ZASYONU".center(70, "â”"))
    
    # Ana tensor oluÅŸtur
    original = torch.arange(12, dtype=torch.float32)  # [0, 1, 2, ..., 11]
    inspect_tensor_anatomy(original, "Orijinal Tensor")
    
    # Storage iÃ§eriÄŸini gÃ¶ster
    print("ğŸ’¾ STORAGE Ä°Ã‡ERÄ°ÄÄ° (Ham Bellek):")
    print(f"Storage Data Pointer: {original.data_ptr()}")
    print(f"Storage iÃ§eriÄŸi: {list(original.storage())}\n")
    
    # View ile yeniden ÅŸekillendirme (AYNI STORAGE)
    reshaped = original.view(3, 4)  # 3x4 matris
    inspect_tensor_anatomy(reshaped, "View ile Yeniden ÅekillendirilmiÅŸ (3x4)")
    
    # âš ï¸ KRÄ°TÄ°K: Her iki tensor de aynÄ± storage'Ä± kullanÄ±yor
    print("ğŸ”´ STORAGE PAYLAÅIMI TESTÄ°:")
    print(f"Orijinal Storage ID: {original.storage().data_ptr()}")
    print(f"Reshaped Storage ID: {reshaped.storage().data_ptr()}")
    print(f"AynÄ± mÄ±? {original.storage().data_ptr() == reshaped.storage().data_ptr()}")
    
    # Slicing ile offset deÄŸiÅŸimi
    sliced = original[3:9]  # Index 3'ten 9'a kadar
    inspect_tensor_anatomy(sliced, "Slice EdilmiÅŸ Tensor [3:9]")
    
    print("ğŸ“ OFFSET FARKI:")
    print(f"Orijinal offset: {original.storage_offset()}")
    print(f"Sliced offset: {sliced.storage_offset()}")
    print(f"âš¡ Slice, storage'da 3. elemandan baÅŸlÄ±yor (zero-copy!)\n")


def demonstrate_stride_mechanism() -> None:
    """
    Stride (adÄ±m) mekanizmasÄ±nÄ± detaylÄ± aÃ§Ä±klar.
    Transpose ve permute iÅŸlemlerinin stride'Ä± nasÄ±l deÄŸiÅŸtirdiÄŸini gÃ¶sterir.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 3: STRIDE MEKANÄ°ZMASI - BELLEK ATLAMALARI".center(70, "â”"))
    
    # 2D tensor oluÅŸtur
    matrix = torch.arange(12, dtype=torch.float32).reshape(3, 4)
    print(f"ğŸ“Š Orijinal Matris (3x4):\n{matrix}")
    inspect_tensor_anatomy(matrix, "Orijinal Matris")
    
    print("ğŸ§® STRIDE HESAPLAMA:")
    print(f"Stride: {matrix.stride()}")
    print(f"  - SatÄ±r deÄŸiÅŸtirmek iÃ§in 4 eleman atla (stride[0]=4)")
    print(f"  - SÃ¼tun deÄŸiÅŸtirmek iÃ§in 1 eleman atla (stride[1]=1)")
    print(f"  - matrix[1,2] konumu = base + 1*4 + 2*1 = 0 + 4 + 2 = 6. eleman")
    print(f"  - DoÄŸrulama: matrix[1,2] = {matrix[1, 2]} (beklenen: 6.0)\n")
    
    # Transpose iÅŸlemi
    transposed = matrix.t()  # veya matrix.transpose(0, 1)
    print(f"ğŸ“Š Transpose EdilmiÅŸ Matris (4x3):\n{transposed}")
    inspect_tensor_anatomy(transposed, "Transpose EdilmiÅŸ Matris")
    
    print("âš ï¸ KRÄ°TÄ°K NOKTA:")
    print(f"Transpose sonrasÄ± stride: {transposed.stride()}")
    print(f"  - Stride ters dÃ¶ndÃ¼: (1, 4) -> ArtÄ±k ROW-MAJOR deÄŸil!")
    print(f"  - Contiguous mu? {transposed.is_contiguous()}")
    print(f"  - Bellekte veri AYNI, sadece eriÅŸim ÅŸekli deÄŸiÅŸti!\n")
    
    # HATA Ã–RNEÄÄ°: Non-contiguous tensor'da view kullanÄ±mÄ±
    print("ğŸ”´ YAYGIN HATA Ã–RNEÄÄ°:")
    try:
        # HATA: Transpose edilmiÅŸ tensor contiguous deÄŸil, view() Ã§alÄ±ÅŸmaz
        wrong_view = transposed.view(12)
        print(f"View baÅŸarÄ±lÄ±: {wrong_view}")
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"ğŸ’¡ Ã‡Ã–ZÃœM: Ã–nce .contiguous() Ã§aÄŸÄ±r!")
        correct_view = transposed.contiguous().view(12)
        print(f"âœ… DoÄŸru kullanÄ±m: {correct_view}\n")


def demonstrate_contiguous_memory() -> None:
    """
    Contiguous (bitiÅŸik) bellek kavramÄ±nÄ± aÃ§Ä±klar.
    .contiguous() metodunun ne zaman gerekli olduÄŸunu gÃ¶sterir.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 4: CONTIGUOUS MEMORY - BÄ°TÄ°ÅÄ°K BELLEK".center(70, "â”"))
    
    # Contiguous tensor
    cont_tensor = torch.arange(6).reshape(2, 3)
    print(f"ğŸ“Š Contiguous Tensor:\n{cont_tensor}")
    print(f"Is contiguous? {cont_tensor.is_contiguous()}")
    print(f"Stride: {cont_tensor.stride()}")
    print(f"Bellekte sÄ±ralama: [0,1,2,3,4,5] (Row-major order)\n")
    
    # Non-contiguous tensor (transpose sonrasÄ±)
    non_cont = cont_tensor.t()
    print(f"ğŸ“Š Non-Contiguous Tensor (Transpose):\n{non_cont}")
    print(f"Is contiguous? {non_cont.is_contiguous()}")
    print(f"Stride: {non_cont.stride()}")
    print(f"Bellekte sÄ±ralama: Hala [0,1,2,3,4,5] ama eriÅŸim farklÄ±!\n")
    
    # Contiguous hale getirme
    made_contiguous = non_cont.contiguous()
    print(f"ğŸ“Š Contiguous YapÄ±lmÄ±ÅŸ Tensor:\n{made_contiguous}")
    print(f"Is contiguous? {made_contiguous.is_contiguous()}")
    print(f"Stride: {made_contiguous.stride()}")
    
    print("âš¡ PERFORMANS ETKÄ°SÄ°:")
    print(f"Non-contiguous data pointer: {non_cont.data_ptr()}")
    print(f"Contiguous data pointer: {made_contiguous.data_ptr()}")
    print(f"FarklÄ± mÄ±? {non_cont.data_ptr() != made_contiguous.data_ptr()}")
    print(f"ğŸ’¡ .contiguous() YENÄ° BELLEK AYIRIR ve veriyi kopyalar!\n")


def demonstrate_memory_efficiency() -> None:
    """
    View vs Clone vs Copy iÅŸlemlerinin bellek kullanÄ±mÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r.
    """
    print("\n" + "ğŸ¯ BÃ–LÃœM 5: VIEW VS CLONE VS COPY - BELLEK VERÄ°MLÄ°LÄ°ÄÄ°".center(70, "â”"))
    
    original = torch.arange(1000000, dtype=torch.float32)  # 1 milyon eleman
    original_size = original.element_size() * original.nelement()
    
    print(f"ğŸ“Š Orijinal Tensor: {original.shape}")
    print(f"ğŸ’¾ Bellek kullanÄ±mÄ±: {original_size / (1024**2):.2f} MB\n")
    
    # VIEW: AynÄ± belleÄŸi paylaÅŸÄ±r
    viewed = original.view(1000, 1000)
    print(f"ğŸ”— VIEW Ä°ÅŸlemi:")
    print(f"  - Yeni shape: {viewed.shape}")
    print(f"  - AynÄ± storage? {original.data_ptr() == viewed.data_ptr()}")
    print(f"  - Ekstra bellek: 0 MB (Zero-copy!)\n")
    
    # CLONE: Yeni bellek ayÄ±rÄ±r, gradient graph korunur
    cloned = original.clone()
    print(f"ğŸ“‹ CLONE Ä°ÅŸlemi:")
    print(f"  - AynÄ± storage? {original.data_ptr() == cloned.data_ptr()}")
    print(f"  - Ekstra bellek: {(cloned.element_size() * cloned.nelement()) / (1024**2):.2f} MB")
    print(f"  - Gradient graph korunur mu? Evet (autograd iÃ§in kullan)\n")
    
    # DETACH + CLONE: Gradient graph kopmaz
    detached = original.detach().clone()
    print(f"âœ‚ï¸ DETACH + CLONE:")
    print(f"  - Gradient graph'tan koptu mu? Evet")
    print(f"  - KullanÄ±m: Inference sÄ±rasÄ±nda bellek tasarrufu\n")


def intentional_bug_demo() -> None:
    """
    Yeni baÅŸlayanlarÄ±n sÄ±k yaptÄ±ÄŸÄ± hatalarÄ± gÃ¶sterir ve dÃ¼zeltir.
    """
    print("\n" + "ğŸ¯ BONUS: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    a = torch.tensor([1, 2, 3], dtype=torch.float32)
    b = torch.tensor([4, 5, 6], dtype=torch.float32)
    
    # HATA 1: Element-wise Ã§arpma vs Dot product
    print("ğŸ”´ HATA 1: Ã‡arpma Ä°ÅŸlemi KarÄ±ÅŸÄ±klÄ±ÄŸÄ±")
    element_wise = a * b  # Element-wise multiplication
    print(f"a * b (Element-wise): {element_wise}")
    
    # DOÄRU: Dot product iÃ§in @ veya torch.dot
    dot_product = a @ b  # veya torch.dot(a, b)
    print(f"a @ b (Dot product): {dot_product}")
    print(f"ğŸ’¡ Fark: * -> [1*4, 2*5, 3*6], @ -> 1*4 + 2*5 + 3*6\n")
    
    # HATA 2: In-place iÅŸlem sonrasÄ± gradient hatasÄ±
    print("ğŸ”´ HATA 2: In-place Ä°ÅŸlem Gradient HatasÄ±")
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    print(f"Orijinal x: {x}")
    
    # YANLIÅ: In-place iÅŸlem gradient graph'Ä± bozar
    # x.add_(1.0)  # Bu satÄ±r aÃ§Ä±lÄ±rsa backward() hatasÄ± verir
    
    # DOÄRU: Yeni tensor dÃ¶ndÃ¼r
    x_new = x.add(1.0)  # veya x = x + 1.0
    print(f"x + 1.0 (DoÄŸru): {x_new}")
    print(f"ğŸ’¡ In-place iÅŸlemler (_ile bitenler) gradient'i bozar!\n")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu - TÃ¼m demolarÄ± sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    print("\n" + "="*70)
    print("ğŸš€ PYTORCH TENSOR MEKANÄ°ÄÄ° - BELLEK DÃœZENÄ° ANALÄ°ZÄ°".center(70))
    print("="*70)
    
    demonstrate_tensor_vs_numpy()
    demonstrate_storage_and_offset()
    demonstrate_stride_mechanism()
    demonstrate_contiguous_memory()
    demonstrate_memory_efficiency()
    intentional_bug_demo()
    
    print("\n" + "="*70)
    print("âœ… DERS 01 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
