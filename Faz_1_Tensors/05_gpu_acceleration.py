"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 05: GPU HIZLANDIRMA - CUDA, CPU-GPU TRANSFER VE BOTTLENECK ANALÄ°ZÄ°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: CUDA Ã§ekirdeklerini anlamak, CPU-GPU veri transferini optimize etmek.
Performans darboÄŸazlarÄ±nÄ± tespit etmek ve Ã§Ã¶zmek.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
import time
from typing import Tuple, List, Optional
import sys


def check_cuda_availability() -> None:
    """
    CUDA kullanÄ±labilirliÄŸini ve GPU bilgilerini gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 1: CUDA KULLANILABIÌ‡LIÌ‡RLIÌ‡ÄžIÌ‡ VE GPU BÄ°Ì‡LGÄ°Ì‡LERÄ°Ì‡".center(70, "â”"))
    
    print(f"\nðŸ” CUDA KullanÄ±labilir mi? {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA Versiyonu: {torch.version.cuda}")
        print(f"âœ… cuDNN Versiyonu: {torch.backends.cudnn.version()}")
        print(f"âœ… GPU SayÄ±sÄ±: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            print(f"\nðŸ“Š GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"   Toplam Bellek: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
            print(f"   CUDA Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}")
            print(f"   Multi-Processor Count: {torch.cuda.get_device_properties(i).multi_processor_count}")
    else:
        print("âš ï¸  CUDA kullanÄ±lamÄ±yor. CPU modunda Ã§alÄ±ÅŸacaÄŸÄ±z.")
        print("ðŸ’¡ Google Colab veya GPU'lu bir makine kullanÄ±n.")


def demonstrate_device_management() -> None:
    """
    Tensor'larÄ± farklÄ± cihazlar arasÄ±nda taÅŸÄ±ma iÅŸlemlerini gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 2: DEVICE MANAGEMENT - CÄ°Ì‡HAZ YÃ–NETÄ°Ì‡MÄ°Ì‡".center(70, "â”"))
    
    # CPU'da tensor oluÅŸturma
    cpu_tensor = torch.randn(3, 4)
    print(f"ðŸ“Š CPU Tensor:")
    print(f"   Device: {cpu_tensor.device}")
    print(f"   Data pointer: {cpu_tensor.data_ptr()}")
    print(f"   Shape: {cpu_tensor.shape}\n")
    
    if torch.cuda.is_available():
        # GPU'ya taÅŸÄ±ma - YÃ¶ntem 1: .to()
        print("â”€"*70)
        print("ðŸ”¹ YÃ–NTEM 1: .to() ile GPU'ya TaÅŸÄ±ma")
        
        start = time.time()
        gpu_tensor_1 = cpu_tensor.to('cuda')
        transfer_time_1 = time.time() - start
        
        print(f"   Device: {gpu_tensor_1.device}")
        print(f"   Data pointer: {gpu_tensor_1.data_ptr()}")
        print(f"   Transfer sÃ¼resi: {transfer_time_1*1000:.4f} ms")
        print(f"   Yeni tensor mi? {cpu_tensor.data_ptr() != gpu_tensor_1.data_ptr()}\n")
        
        # GPU'ya taÅŸÄ±ma - YÃ¶ntem 2: .cuda()
        print("â”€"*70)
        print("ðŸ”¹ YÃ–NTEM 2: .cuda() ile GPU'ya TaÅŸÄ±ma")
        
        gpu_tensor_2 = cpu_tensor.cuda()
        print(f"   Device: {gpu_tensor_2.device}")
        print(f"   .to('cuda') ile aynÄ± mÄ±? {torch.equal(gpu_tensor_1, gpu_tensor_2)}\n")
        
        # Belirli GPU'ya taÅŸÄ±ma (Multi-GPU sistemlerde)
        if torch.cuda.device_count() > 1:
            print("â”€"*70)
            print("ðŸ”¹ MULTI-GPU: Belirli GPU'ya TaÅŸÄ±ma")
            
            gpu_0 = cpu_tensor.to('cuda:0')
            gpu_1 = cpu_tensor.to('cuda:1')
            
            print(f"   GPU 0: {gpu_0.device}")
            print(f"   GPU 1: {gpu_1.device}\n")
        
        # CPU'ya geri taÅŸÄ±ma
        print("â”€"*70)
        print("ðŸ”¹ GPU'dan CPU'ya Geri TaÅŸÄ±ma")
        
        back_to_cpu = gpu_tensor_1.cpu()
        print(f"   Device: {back_to_cpu.device}")
        print(f"   Orijinal ile aynÄ± deÄŸerler mi? {torch.equal(cpu_tensor, back_to_cpu)}\n")
        
        # âš ï¸ KRÄ°TÄ°K: FarklÄ± cihazlardaki tensor'lar iÅŸlem yapamaz
        print("â”€"*70)
        print("ðŸ”´ HATA: FarklÄ± Cihazlardaki Tensor'lar")
        
        try:
            # YANLIÅž: CPU ve GPU tensor'larÄ± toplanamaz
            result = cpu_tensor + gpu_tensor_1
        except RuntimeError as e:
            print(f"   âŒ HATA: {e}")
            print(f"   ðŸ’¡ Ã‡Ã–ZÃœM: Her iki tensor'u da aynÄ± cihaza taÅŸÄ±")
            result = cpu_tensor.to('cuda') + gpu_tensor_1
            print(f"   âœ… DoÄŸru: cpu_tensor.to('cuda') + gpu_tensor_1")
    else:
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")


def demonstrate_performance_comparison() -> None:
    """
    CPU vs GPU performans karÅŸÄ±laÅŸtÄ±rmasÄ± yapar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 3: PERFORMANS KARÅžILAÅžTIRMASI - CPU VS GPU".center(70, "â”"))
    
    sizes = [100, 500, 1000, 2000, 4000]
    
    print(f"\n{'Size':>6} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>10}")
    print("â”€"*50)
    
    for size in sizes:
        # CPU matris Ã§arpÄ±mÄ±
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        
        start = time.time()
        c_cpu = a_cpu @ b_cpu
        cpu_time = (time.time() - start) * 1000
        
        if torch.cuda.is_available():
            # GPU matris Ã§arpÄ±mÄ±
            a_gpu = a_cpu.to('cuda')
            b_gpu = b_cpu.to('cuda')
            
            # Warm-up (GPU'yu Ä±sÄ±t)
            _ = a_gpu @ b_gpu
            torch.cuda.synchronize()  # GPU iÅŸlemlerini bekle
            
            start = time.time()
            c_gpu = a_gpu @ b_gpu
            torch.cuda.synchronize()
            gpu_time = (time.time() - start) * 1000
            
            speedup = cpu_time / gpu_time
            print(f"{size:>6} | {cpu_time:>10.4f} | {gpu_time:>10.4f} | {speedup:>10.2f}x")
        else:
            print(f"{size:>6} | {cpu_time:>10.4f} | {'N/A':>10} | {'N/A':>10}")
    
    if torch.cuda.is_available():
        print(f"\nðŸ’¡ BÃ¼yÃ¼k matrisler iÃ§in GPU {speedup:.0f}x daha hÄ±zlÄ±!")
    else:
        print(f"\nâš ï¸  GPU yok, karÅŸÄ±laÅŸtÄ±rma yapÄ±lamadÄ±.")


def demonstrate_memory_transfer_bottleneck() -> None:
    """
    CPU-GPU veri transferinin performans darboÄŸazÄ± olduÄŸunu gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 4: BELLEK TRANSFER DARBOÄžAZI - CPU â†” GPU".center(70, "â”"))
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")
        return
    
    size = 4096
    
    # Senaryo 1: Her iterasyonda CPU â†’ GPU transfer (KÃ–TÃœ)
    print("ðŸ”´ KÃ–TÃœ PRATÄ°K: Her Ä°terasyonda Transfer")
    
    total_time = 0
    for i in range(10):
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        
        start = time.time()
        a_gpu = a_cpu.to('cuda')  # Transfer!
        b_gpu = b_cpu.to('cuda')  # Transfer!
        c_gpu = a_gpu @ b_gpu
        c_cpu = c_gpu.cpu()       # Transfer!
        torch.cuda.synchronize()
        total_time += time.time() - start
    
    bad_time = total_time * 1000
    print(f"   10 iterasyon: {bad_time:.2f} ms")
    print(f"   Her iterasyon: {bad_time/10:.2f} ms\n")
    
    # Senaryo 2: Veriyi GPU'da tut (Ä°YÄ°)
    print("â”€"*70)
    print("âœ… Ä°YÄ° PRATÄ°K: Veriyi GPU'da Tut")
    
    a_gpu = torch.randn(size, size, device='cuda')  # DoÄŸrudan GPU'da oluÅŸtur
    b_gpu = torch.randn(size, size, device='cuda')
    
    torch.cuda.synchronize()
    start = time.time()
    for i in range(10):
        c_gpu = a_gpu @ b_gpu
    torch.cuda.synchronize()
    good_time = (time.time() - start) * 1000
    
    print(f"   10 iterasyon: {good_time:.2f} ms")
    print(f"   Her iterasyon: {good_time/10:.2f} ms")
    
    speedup = bad_time / good_time
    print(f"\nðŸš€ HIZ ARTIÅžI: {speedup:.1f}x daha hÄ±zlÄ±!")
    print(f"ðŸ’¡ Sebep: CPU-GPU transfer overhead'i yok")


def demonstrate_pinned_memory() -> None:
    """
    Pinned memory (page-locked memory) kullanÄ±mÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 5: PINNED MEMORY - HIZLI TRANSFER".center(70, "â”"))
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")
        return
    
    size = (1000, 1000)
    
    # Normal CPU tensor
    print("ðŸ”¹ Normal CPU Tensor â†’ GPU Transfer")
    
    normal_tensor = torch.randn(*size)
    
    start = time.time()
    for _ in range(100):
        _ = normal_tensor.to('cuda')
    torch.cuda.synchronize()
    normal_time = (time.time() - start) * 1000
    
    print(f"   100 transfer: {normal_time:.2f} ms\n")
    
    # Pinned memory tensor
    print("â”€"*70)
    print("ðŸ”¹ Pinned Memory Tensor â†’ GPU Transfer")
    
    pinned_tensor = torch.randn(*size).pin_memory()
    
    start = time.time()
    for _ in range(100):
        _ = pinned_tensor.to('cuda', non_blocking=True)
    torch.cuda.synchronize()
    pinned_time = (time.time() - start) * 1000
    
    print(f"   100 transfer: {pinned_time:.2f} ms")
    
    speedup = normal_time / pinned_time
    print(f"\nðŸš€ HIZ ARTIÅžI: {speedup:.2f}x daha hÄ±zlÄ±!")
    print(f"ðŸ’¡ Pinned memory, DMA (Direct Memory Access) kullanÄ±r")
    print(f"âš ï¸  Dikkat: Pinned memory sistem RAM'ini kilitler, fazla kullanma!")


def demonstrate_cuda_streams() -> None:
    """
    CUDA streams ile paralel iÅŸlem yapmayÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 6: CUDA STREAMS - PARALEL Ä°ÅžLEM".center(70, "â”"))
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")
        return
    
    size = 2048
    
    # Senaryo 1: SÄ±ralÄ± iÅŸlem (default stream)
    print("ðŸ”¹ SÄ±ralÄ± Ä°ÅŸlem (Default Stream)")
    
    a = torch.randn(size, size, device='cuda')
    b = torch.randn(size, size, device='cuda')
    c = torch.randn(size, size, device='cuda')
    
    torch.cuda.synchronize()
    start = time.time()
    
    result1 = a @ b
    result2 = b @ c
    result3 = a @ c
    
    torch.cuda.synchronize()
    sequential_time = (time.time() - start) * 1000
    
    print(f"   3 matris Ã§arpÄ±mÄ±: {sequential_time:.2f} ms\n")
    
    # Senaryo 2: Paralel iÅŸlem (multiple streams)
    print("â”€"*70)
    print("ðŸ”¹ Paralel Ä°ÅŸlem (Multiple Streams)")
    
    stream1 = torch.cuda.Stream()
    stream2 = torch.cuda.Stream()
    stream3 = torch.cuda.Stream()
    
    torch.cuda.synchronize()
    start = time.time()
    
    with torch.cuda.stream(stream1):
        result1 = a @ b
    
    with torch.cuda.stream(stream2):
        result2 = b @ c
    
    with torch.cuda.stream(stream3):
        result3 = a @ c
    
    torch.cuda.synchronize()
    parallel_time = (time.time() - start) * 1000
    
    print(f"   3 matris Ã§arpÄ±mÄ±: {parallel_time:.2f} ms")
    
    speedup = sequential_time / parallel_time
    print(f"\nðŸš€ HIZ ARTIÅžI: {speedup:.2f}x daha hÄ±zlÄ±!")
    print(f"ðŸ’¡ BaÄŸÄ±msÄ±z iÅŸlemler paralel Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±")


def demonstrate_memory_management() -> None:
    """
    GPU bellek yÃ¶netimi ve optimizasyon tekniklerini gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 7: GPU BELLEK YÃ–NETÄ°MÄ°".center(70, "â”"))
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")
        return
    
    # Bellek durumunu gÃ¶ster
    print("ðŸ”¹ GPU Bellek Durumu")
    
    allocated = torch.cuda.memory_allocated() / 1024**2
    reserved = torch.cuda.memory_reserved() / 1024**2
    
    print(f"   Allocated: {allocated:.2f} MB")
    print(f"   Reserved: {reserved:.2f} MB\n")
    
    # BÃ¼yÃ¼k tensor oluÅŸtur
    print("â”€"*70)
    print("ðŸ”¹ BÃ¼yÃ¼k Tensor OluÅŸturma")
    
    big_tensor = torch.randn(10000, 10000, device='cuda')
    
    allocated_after = torch.cuda.memory_allocated() / 1024**2
    print(f"   Tensor boyutu: {big_tensor.element_size() * big_tensor.nelement() / 1024**2:.2f} MB")
    print(f"   Allocated: {allocated_after:.2f} MB (+{allocated_after - allocated:.2f} MB)\n")
    
    # BelleÄŸi temizle
    print("â”€"*70)
    print("ðŸ”¹ Bellek Temizleme")
    
    del big_tensor
    torch.cuda.empty_cache()
    
    allocated_cleaned = torch.cuda.memory_allocated() / 1024**2
    reserved_cleaned = torch.cuda.memory_reserved() / 1024**2
    
    print(f"   Allocated: {allocated_cleaned:.2f} MB")
    print(f"   Reserved: {reserved_cleaned:.2f} MB")
    print(f"   ðŸ’¡ empty_cache() reserved memory'yi serbest bÄ±raktÄ±\n")
    
    # Bellek profiling
    print("â”€"*70)
    print("ðŸ”¹ Bellek Profiling")
    
    print(f"   Max allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB")
    print(f"   Max reserved: {torch.cuda.max_memory_reserved() / 1024**2:.2f} MB")
    
    # Reset statistics
    torch.cuda.reset_peak_memory_stats()
    print(f"   ðŸ’¡ reset_peak_memory_stats() ile istatistikler sÄ±fÄ±rlandÄ±")


def demonstrate_common_pitfalls() -> None:
    """
    GPU kullanÄ±mÄ±nda sÄ±k yapÄ±lan hatalarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 8: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA yok, bu bÃ¶lÃ¼m atlanÄ±yor.")
        return
    
    # HATA 1: synchronize() unutmak
    print("ðŸ”´ HATA 1: torch.cuda.synchronize() Unutmak")
    
    a = torch.randn(1000, 1000, device='cuda')
    b = torch.randn(1000, 1000, device='cuda')
    
    # YANLIÅž: GPU iÅŸlemi asenkron, zaman Ã¶lÃ§Ã¼mÃ¼ yanlÄ±ÅŸ
    start = time.time()
    c = a @ b
    wrong_time = (time.time() - start) * 1000
    
    # DOÄžRU: synchronize() ile bekle
    start = time.time()
    c = a @ b
    torch.cuda.synchronize()
    correct_time = (time.time() - start) * 1000
    
    print(f"   Synchronize olmadan: {wrong_time:.6f} ms (YANLIÅž!)")
    print(f"   Synchronize ile: {correct_time:.4f} ms (DOÄžRU)")
    print(f"   ðŸ’¡ GPU iÅŸlemleri asenkron, mutlaka synchronize() kullan!\n")
    
    # HATA 2: Gereksiz CPU-GPU transfer
    print("â”€"*70)
    print("ðŸ”´ HATA 2: Gereksiz CPU-GPU Transfer")
    
    # YANLIÅž: Her iterasyonda .item() Ã§aÄŸÄ±rma
    loss_values = []
    tensor = torch.randn(1000, device='cuda')
    
    start = time.time()
    for i in range(1000):
        loss = tensor.sum()
        loss_values.append(loss.item())  # CPU'ya transfer!
    wrong_time = (time.time() - start) * 1000
    
    # DOÄžRU: GPU'da topla, sonra bir kez transfer et
    start = time.time()
    losses = []
    for i in range(1000):
        loss = tensor.sum()
        losses.append(loss)
    
    loss_values = [l.item() for l in losses]  # Tek seferde
    correct_time = (time.time() - start) * 1000
    
    print(f"   Her iterasyonda .item(): {wrong_time:.2f} ms")
    print(f"   Sonunda toplu .item(): {correct_time:.2f} ms")
    print(f"   ðŸš€ {wrong_time/correct_time:.1f}x daha hÄ±zlÄ±!")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ðŸš€ GPU HIZLANDIRMA - CUDA VE PERFORMANS OPTÄ°MÄ°ZASYONU".center(70))
    print("="*70)
    
    check_cuda_availability()
    demonstrate_device_management()
    demonstrate_performance_comparison()
    demonstrate_memory_transfer_bottleneck()
    demonstrate_pinned_memory()
    demonstrate_cuda_streams()
    demonstrate_memory_management()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 05 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
