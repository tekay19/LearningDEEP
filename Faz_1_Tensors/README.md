# ğŸ”§ FAZ 1: TENSORS & COMPUTATIONAL GRAPH (THE ENGINE)

## âœ… TamamlandÄ±!

Bu faz, PyTorch'un temelini oluÅŸturan **tensor mekanikleri** ve **otomatik tÃ¼rev sistemi**ni kapsar. "Under the Hood" seviyesinde, bellekten GPU'ya, stride'dan autograd'a kadar her ÅŸeyi Ã¶ÄŸrendiniz.

---

## ğŸ“š Dersler

| # | Dosya | Konu | Durum |
|---|-------|------|-------|
| **01** | `01_tensor_mechanics.py` | Tensor vs NumPy, Storage, Offset, Stride | âœ… |
| **02** | `02_tensor_math_gemm.py` | GEMM, Broadcasting, Vectorization | âœ… |
| **03** | `03_indexing_advanced.py` | Masking, Fancy Indexing, View vs Copy | âœ… |
| **04** | `04_manipulation_view_reshape.py` | view(), reshape(), permute(), transpose() | âœ… |
| **05** | `05_gpu_acceleration.py` | CUDA, CPU-GPU Transfer, Bottleneck | âœ… |
| **06** | `06_autograd_engine.py` | DAG, .backward(), Gradient Flow | âœ… |
| **07** | `07_custom_autograd.py` | torch.autograd.Function, Custom Derivatives | âœ… |

---

## ğŸ¯ Ã–ÄŸrendikleriniz

### ğŸ§  Kavramsal Bilgi
- âœ… **Tensor Anatomy:** Storage, Offset, Stride, Contiguous Memory
- âœ… **GEMM Optimization:** BLAS/cuBLAS, Cache Blocking, Tensor Cores
- âœ… **Broadcasting Rules:** Otomatik boyut geniÅŸletme mekanizmasÄ±
- âœ… **View vs Copy:** Bellek paylaÅŸÄ±mÄ± ve optimizasyon
- âœ… **GPU Programming:** CUDA kernels, Streams, Pinned Memory
- âœ… **Autograd Engine:** DAG yapÄ±sÄ±, Backward propagation
- âœ… **Custom Gradients:** torch.autograd.Function ile Ã¶zel tÃ¼revler

### ğŸ’» Pratik Beceriler
- âœ… Tensor'larÄ±n bellekte nasÄ±l yerleÅŸtiÄŸini analiz etme
- âœ… CPU-GPU transfer darboÄŸazlarÄ±nÄ± tespit etme ve Ã§Ã¶zme
- âœ… Gradient flow'u debug etme
- âœ… Kendi activation function'larÄ±nÄ±zÄ± yazma
- âœ… Numerical gradient checking ile doÄŸrulama

### ğŸ­ Production Bilgisi
- âœ… Memory leak'leri Ã¶nleme
- âœ… ONNX export sorunlarÄ±nÄ± Ã§Ã¶zme
- âœ… Mixed precision training (FP16/FP32)
- âœ… Gradient accumulation stratejileri
- âœ… Inference optimization (no_grad vs inference_mode)

---

## ğŸš€ HÄ±zlÄ± Test

TÃ¼m dersleri Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
cd Faz_1_Tensors

# Her dersi sÄ±rayla Ã§alÄ±ÅŸtÄ±r
python 01_tensor_mechanics.py
python 02_tensor_math_gemm.py
python 03_indexing_advanced.py
python 04_manipulation_view_reshape.py
python 05_gpu_acceleration.py
python 06_autograd_engine.py
python 07_custom_autograd.py
```

---

## ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmalarÄ±

Bu fazda Ã¶ÄŸrendiÄŸiniz optimizasyonlarÄ±n etkisi:

| Optimizasyon | HÄ±z ArtÄ±ÅŸÄ± | Bellek Tasarrufu |
|--------------|------------|------------------|
| View vs Clone | âˆ (Zero-copy) | %100 |
| GEMM (BLAS vs Naive) | 1000x+ | - |
| GPU vs CPU (BÃ¼yÃ¼k matris) | 100-500x | - |
| Pinned Memory | 2-3x | - |
| CUDA Streams | 2-4x | - |
| no_grad() | 1.5-2x | %50 |
| Gradient Checkpointing | 0.7x (yavaÅŸ) | %50 |

---

## ğŸ“ Ã–nemli Kavramlar

### 1ï¸âƒ£ Contiguous Memory
```python
# Non-contiguous
x = torch.randn(3, 4)
y = x.t()  # Transpose
print(y.is_contiguous())  # False

# Contiguous yap
y = y.contiguous()  # Yeni bellek ayÄ±rÄ±r!
```

### 2ï¸âƒ£ GPU Transfer Optimization
```python
# KÃ–TÃœ: Her iterasyonda transfer
for i in range(1000):
    x_gpu = x_cpu.to('cuda')
    y = model(x_gpu)
    loss = y.cpu()

# Ä°YÄ°: Veriyi GPU'da tut
x_gpu = x_cpu.to('cuda')
for i in range(1000):
    y = model(x_gpu)
```

### 3ï¸âƒ£ Gradient Accumulation
```python
# YANLIÅ: Gradientler birikiyor
for epoch in range(10):
    loss = model(x)
    loss.backward()
    optimizer.step()

# DOÄRU: Her iterasyonda sÄ±fÄ±rla
for epoch in range(10):
    optimizer.zero_grad()
    loss = model(x)
    loss.backward()
    optimizer.step()
```

### 4ï¸âƒ£ Custom Autograd
```python
class MyFunction(Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)  # Kaydet!
        return input * 2
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return grad_output * 2  # TÃ¼rev
```

---

## âš ï¸ YaygÄ±n Hatalar

### âŒ HATA 1: View'da Non-Contiguous
```python
x = torch.randn(3, 4)
y = x.t()
z = y.view(12)  # RuntimeError!

# Ã‡Ã–ZÃœM
z = y.contiguous().view(12)
```

### âŒ HATA 2: GPU Synchronize Unutmak
```python
# YANLIÅ
start = time.time()
y = x_gpu @ x_gpu
time_taken = time.time() - start  # YanlÄ±ÅŸ!

# DOÄRU
start = time.time()
y = x_gpu @ x_gpu
torch.cuda.synchronize()  # Bekle!
time_taken = time.time() - start
```

### âŒ HATA 3: In-place Ä°ÅŸlem Gradient Graph'Ä± Bozar
```python
x = torch.tensor([1.0], requires_grad=True)
y = x**2

y.add_(1.0)  # RuntimeError!
y.backward()

# Ã‡Ã–ZÃœM
y = y.add(1.0)  # Yeni tensor dÃ¶ndÃ¼r
```

---

## ğŸ“– Ek Kaynaklar

### Resmi DokÃ¼mantasyon
- [PyTorch Tensor Internals](http://blog.ezyang.com/2019/05/pytorch-internals/)
- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)

### Akademik Makaleler
- "Automatic Differentiation in PyTorch" (Paszke et al., 2017)
- "Anatomy of High-Performance Matrix Multiplication" (Goto & Van De Geijn)
- "CUDA Programming Guide" (NVIDIA)

### Video Kaynaklar
- Andrej Karpathy - "Neural Networks: Zero to Hero"
- PyTorch Internals - Edward Yang
- CUDA Programming - NVIDIA Developer

---

## ğŸ¯ Sonraki AdÄ±m

**Faz 2: Neural Network Fundamentals**

ArtÄ±k tensor mekanikleri ve autograd'Ä± biliyorsunuz. SÄ±rada:
- Linear Regression (sÄ±fÄ±rdan, matematik ile)
- nn.Module mimarisi
- Activation functions (ReLU, GELU, Swish)
- Loss functions (CrossEntropy, MSE)
- Optimizers (SGD, Adam, AdamW)

```bash
cd ../Faz_2_Neural_Networks
```

---

## âœ… BaÅŸarÄ± Kriterleri

Bu fazÄ± tamamladÄ±ysanÄ±z:

- [ ] Bir tensor'un stride'Ä±nÄ± hesaplayabiliyorsunuz
- [ ] View vs reshape farkÄ±nÄ± aÃ§Ä±klayabiliyorsunuz
- [ ] CPU-GPU transfer darboÄŸazÄ±nÄ± tespit edebiliyorsunuz
- [ ] Gradient flow'u debug edebiliyorsunuz
- [ ] Kendi activation function'Ä±nÄ±zÄ± yazabiliyorsunuz
- [ ] Numerical gradient checking yapabiliyorsunuz

**Hepsini iÅŸaretlediyseniz, Faz 2'ye geÃ§ebilirsiniz!** ğŸš€

---

**Son GÃ¼ncelleme:** 18 Ocak 2026  
**Durum:** âœ… TamamlandÄ± (7/7 ders)
