"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 04: TENSOR MANÄ°PÃœLASYONU - VIEW, RESHAPE, PERMUTE, TRANSPOSE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: Tensor ÅŸekil deÄŸiÅŸtirme iÅŸlemlerini derinlemesine anlamak.
Contiguous bellek sorunsalÄ±nÄ± Ã§Ã¶zmek.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
from typing import Tuple, List
import time


def demonstrate_view_vs_reshape() -> None:
    """
    view() ve reshape() arasÄ±ndaki kritik farklarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 1: VIEW VS RESHAPE - NE ZAMAN HANGÄ°SÄ°?".center(70, "â”"))
    
    # Contiguous tensor
    tensor = torch.arange(12, dtype=torch.float32).reshape(3, 4)
    print(f"ðŸ“Š Orijinal Tensor (3Ã—4):\n{tensor}")
    print(f"Is contiguous: {tensor.is_contiguous()}")
    print(f"Stride: {tensor.stride()}\n")
    
    # VIEW: Contiguous tensor'da Ã§alÄ±ÅŸÄ±r
    print("â”€"*70)
    print("âœ… VIEW - Contiguous Tensor'da Ã‡alÄ±ÅŸÄ±r")
    
    viewed = tensor.view(4, 3)
    print(f"tensor.view(4, 3):\n{viewed}")
    print(f"Storage paylaÅŸÄ±mÄ±: {tensor.data_ptr() == viewed.data_ptr()}")
    print(f"Stride: {viewed.stride()}\n")
    
    # Non-contiguous tensor
    print("â”€"*70)
    print("ðŸ”´ VIEW - Non-Contiguous Tensor'da HATA")
    
    transposed = tensor.t()
    print(f"Transposed tensor:\n{transposed}")
    print(f"Is contiguous: {transposed.is_contiguous()}")
    print(f"Stride: {transposed.stride()}")
    
    try:
        # HATA: Non-contiguous tensor'da view() Ã§alÄ±ÅŸmaz
        wrong_view = transposed.view(12)
    except RuntimeError as e:
        print(f"\nâŒ HATA: {e}")
        print(f"\nðŸ’¡ Ã‡Ã–ZÃœM 1: .contiguous() kullan")
        correct_view = transposed.contiguous().view(12)
        print(f"transposed.contiguous().view(12): {correct_view}")
        print(f"Yeni storage oluÅŸturuldu: {tensor.data_ptr() != correct_view.data_ptr()}")
    
    # RESHAPE: Her zaman Ã§alÄ±ÅŸÄ±r
    print("\n" + "â”€"*70)
    print("âœ… RESHAPE - Her Zaman Ã‡alÄ±ÅŸÄ±r")
    
    reshaped = transposed.reshape(12)
    print(f"transposed.reshape(12): {reshaped}")
    print(f"Storage paylaÅŸÄ±mÄ±: {transposed.data_ptr() == reshaped.data_ptr()}")
    print(f"\nðŸ’¡ reshape() gerekirse otomatik .contiguous() Ã§aÄŸÄ±rÄ±r")
    
    # Performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("\n" + "â”€"*70)
    print("â±ï¸  PERFORMANS KARÅžILAÅžTIRMASI")
    
    big_tensor = torch.randn(1000, 1000)
    
    # view() - Zero-copy
    start = time.time()
    for _ in range(10000):
        _ = big_tensor.view(1000000)
    view_time = time.time() - start
    
    # reshape() - Contiguous tensor'da zero-copy
    start = time.time()
    for _ in range(10000):
        _ = big_tensor.reshape(1000000)
    reshape_time = time.time() - start
    
    print(f"view() 10000 kez:    {view_time:.6f}s")
    print(f"reshape() 10000 kez: {reshape_time:.6f}s")
    print(f"Fark: ~{abs(view_time - reshape_time):.6f}s (Ä°hmal edilebilir)")


def demonstrate_permute_and_transpose() -> None:
    """
    permute() ve transpose() iÅŸlemlerini detaylÄ± aÃ§Ä±klar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 2: PERMUTE VE TRANSPOSE - BOYUT YER DEÄžÄ°ÅžTÄ°RME".center(70, "â”"))
    
    # 3D tensor
    tensor = torch.arange(24).reshape(2, 3, 4)
    print(f"ðŸ“Š Orijinal Tensor (2Ã—3Ã—4):\n{tensor}")
    print(f"Shape: {tensor.shape}")
    print(f"Stride: {tensor.stride()}\n")
    
    # TRANSPOSE: Ä°ki boyutu deÄŸiÅŸtir
    print("â”€"*70)
    print("ðŸ”¹ TRANSPOSE - Ä°ki Boyut DeÄŸiÅŸtir")
    
    transposed = tensor.transpose(0, 2)  # Dim 0 ve 2'yi deÄŸiÅŸtir
    print(f"tensor.transpose(0, 2):")
    print(f"Shape: {tensor.shape} â†’ {transposed.shape}")
    print(f"Stride: {tensor.stride()} â†’ {transposed.stride()}")
    print(f"Is contiguous: {transposed.is_contiguous()}")
    print(f"Storage paylaÅŸÄ±mÄ±: {tensor.data_ptr() == transposed.data_ptr()}\n")
    
    # PERMUTE: TÃ¼m boyutlarÄ± yeniden sÄ±rala
    print("â”€"*70)
    print("ðŸ”¹ PERMUTE - TÃ¼m BoyutlarÄ± Yeniden SÄ±rala")
    
    permuted = tensor.permute(2, 0, 1)  # (2,3,4) â†’ (4,2,3)
    print(f"tensor.permute(2, 0, 1):")
    print(f"Shape: {tensor.shape} â†’ {permuted.shape}")
    print(f"Stride: {tensor.stride()} â†’ {permuted.stride()}")
    print(f"Is contiguous: {permuted.is_contiguous()}")
    
    # Stride hesaplama doÄŸrulamasÄ±
    print(f"\nðŸ§® STRIDE HESAPLAMA:")
    print(f"Orijinal stride: {tensor.stride()} â†’ (12, 4, 1)")
    print(f"  - Dim 0: 3Ã—4 = 12 eleman atla")
    print(f"  - Dim 1: 4 eleman atla")
    print(f"  - Dim 2: 1 eleman atla")
    print(f"\nPermute sonrasÄ±: {permuted.stride()} â†’ (1, 12, 4)")
    print(f"  - Yeni dim 0 (eski dim 2): stride = 1")
    print(f"  - Yeni dim 1 (eski dim 0): stride = 12")
    print(f"  - Yeni dim 2 (eski dim 1): stride = 4")
    
    # Pratik Ã¶rnek: Image tensor (NCHW â†’ NHWC)
    print("\n" + "â”€"*70)
    print("ðŸ”¹ PRATÄ°K Ã–RNEK: Image Tensor DÃ¶nÃ¼ÅŸÃ¼mÃ¼")
    
    # PyTorch format: (Batch, Channels, Height, Width)
    image_nchw = torch.randn(32, 3, 224, 224)
    print(f"PyTorch format (NCHW): {image_nchw.shape}")
    
    # TensorFlow format: (Batch, Height, Width, Channels)
    image_nhwc = image_nchw.permute(0, 2, 3, 1)
    print(f"TensorFlow format (NHWC): {image_nhwc.shape}")
    print(f"Is contiguous: {image_nhwc.is_contiguous()}")
    print(f"\nðŸ’¡ ONNX export iÃ§in .contiguous() gerekebilir!")


def demonstrate_squeeze_and_unsqueeze() -> None:
    """
    squeeze() ve unsqueeze() ile boyut ekleme/Ã§Ä±karma.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 3: SQUEEZE VE UNSQUEEZE - BOYUT EKLEME/Ã‡IKARMA".center(70, "â”"))
    
    # UNSQUEEZE: Boyut ekle
    print("ðŸ”¹ UNSQUEEZE - Boyut Ekle")
    
    tensor = torch.tensor([1, 2, 3, 4])
    print(f"Orijinal: {tensor.shape}")
    
    unsqueezed_0 = tensor.unsqueeze(0)
    print(f"unsqueeze(0): {unsqueezed_0.shape} â†’ {unsqueezed_0}")
    
    unsqueezed_1 = tensor.unsqueeze(1)
    print(f"unsqueeze(1): {unsqueezed_1.shape} â†’\n{unsqueezed_1}")
    
    unsqueezed_neg = tensor.unsqueeze(-1)
    print(f"unsqueeze(-1): {unsqueezed_neg.shape} (Son boyuta ekle)\n")
    
    # SQUEEZE: 1 boyutundaki dimensionlarÄ± kaldÄ±r
    print("â”€"*70)
    print("ðŸ”¹ SQUEEZE - 1 Boyutundaki DimensionlarÄ± KaldÄ±r")
    
    tensor_with_ones = torch.randn(1, 3, 1, 5, 1)
    print(f"Orijinal: {tensor_with_ones.shape}")
    
    squeezed_all = tensor_with_ones.squeeze()
    print(f"squeeze() (tÃ¼mÃ¼): {squeezed_all.shape}")
    
    squeezed_dim = tensor_with_ones.squeeze(0)
    print(f"squeeze(0): {squeezed_dim.shape}")
    
    squeezed_dim2 = tensor_with_ones.squeeze(2)
    print(f"squeeze(2): {squeezed_dim2.shape}\n")
    
    # Pratik kullanÄ±m: Batch dimension ekleme
    print("â”€"*70)
    print("ðŸ”¹ PRATÄ°K KULLANIM: Batch Dimension")
    
    single_image = torch.randn(3, 224, 224)  # (C, H, W)
    print(f"Tek gÃ¶rÃ¼ntÃ¼: {single_image.shape}")
    
    batched = single_image.unsqueeze(0)  # (1, C, H, W)
    print(f"Batch'e eklendi: {batched.shape}")
    print(f"ðŸ’¡ Model'e tek gÃ¶rÃ¼ntÃ¼ gÃ¶ndermek iÃ§in gerekli!")


def demonstrate_flatten_and_unflatten() -> None:
    """
    flatten() ve unflatten() ile tensor dÃ¼zleÅŸtirme.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 4: FLATTEN VE UNFLATTEN - DÃœZLEÅžTIRME".center(70, "â”"))
    
    # FLATTEN
    print("ðŸ”¹ FLATTEN - Tensor DÃ¼zleÅŸtirme")
    
    tensor = torch.arange(24).reshape(2, 3, 4)
    print(f"Orijinal (2Ã—3Ã—4):\n{tensor}\n")
    
    # TÃ¼m boyutlarÄ± dÃ¼zleÅŸtir
    flat_all = tensor.flatten()
    print(f"flatten(): {flat_all.shape}")
    print(f"SonuÃ§: {flat_all}\n")
    
    # Belirli boyutlarÄ± dÃ¼zleÅŸtir
    flat_partial = tensor.flatten(start_dim=1)
    print(f"flatten(start_dim=1): {flat_partial.shape}")
    print(f"SonuÃ§:\n{flat_partial}")
    print(f"ðŸ’¡ Ä°lk boyut korundu, geri kalanlar dÃ¼zleÅŸtirildi\n")
    
    # UNFLATTEN (PyTorch 1.13+)
    print("â”€"*70)
    print("ðŸ”¹ UNFLATTEN - DÃ¼zleÅŸtirilmiÅŸ Tensor'u Geri Al")
    
    flat = torch.arange(24)
    print(f"DÃ¼zleÅŸtirilmiÅŸ: {flat.shape}")
    
    unflat = flat.unflatten(0, (2, 3, 4))
    print(f"unflatten(0, (2,3,4)): {unflat.shape}")
    print(f"SonuÃ§:\n{unflat}\n")
    
    # CNN'de kullanÄ±m
    print("â”€"*70)
    print("ðŸ”¹ PRATÄ°K: CNN â†’ Fully Connected GeÃ§iÅŸi")
    
    # Conv layer Ã§Ä±ktÄ±sÄ±: (Batch, Channels, H, W)
    conv_output = torch.randn(32, 512, 7, 7)
    print(f"Conv output: {conv_output.shape}")
    
    # Fully connected iÃ§in dÃ¼zleÅŸtir
    fc_input = conv_output.flatten(start_dim=1)
    print(f"FC input: {fc_input.shape}")
    print(f"ðŸ’¡ Batch dimension korundu, geri kalanlar dÃ¼zleÅŸtirildi")


def demonstrate_advanced_manipulations() -> None:
    """
    GeliÅŸmiÅŸ manipÃ¼lasyon teknikleri.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 5: GELÄ°ÅžMÄ°Åž MANÄ°PÃœLASYONLAR".center(70, "â”"))
    
    # CHUNK: Tensor'u parÃ§alara bÃ¶l
    print("ðŸ”¹ CHUNK - Tensor'u EÅŸit ParÃ§alara BÃ¶l")
    
    tensor = torch.arange(12).reshape(3, 4)
    print(f"Orijinal:\n{tensor}\n")
    
    chunks = tensor.chunk(2, dim=0)  # 2 parÃ§aya bÃ¶l (dim=0)
    print(f"chunk(2, dim=0): {len(chunks)} parÃ§a")
    for i, chunk in enumerate(chunks):
        print(f"ParÃ§a {i}: {chunk.shape}\n{chunk}\n")
    
    # SPLIT: Tensor'u belirli boyutlarda bÃ¶l
    print("â”€"*70)
    print("ðŸ”¹ SPLIT - Tensor'u Belirli Boyutlarda BÃ¶l")
    
    splits = tensor.split([1, 2], dim=0)  # 1 ve 2 satÄ±rlÄ±k parÃ§alar
    print(f"split([1, 2], dim=0): {len(splits)} parÃ§a")
    for i, split in enumerate(splits):
        print(f"ParÃ§a {i}: {split.shape}\n{split}\n")
    
    # CAT: Tensor'larÄ± birleÅŸtir
    print("â”€"*70)
    print("ðŸ”¹ CAT - Tensor'larÄ± BirleÅŸtir")
    
    t1 = torch.tensor([[1, 2], [3, 4]])
    t2 = torch.tensor([[5, 6], [7, 8]])
    
    cat_dim0 = torch.cat([t1, t2], dim=0)
    print(f"cat([t1, t2], dim=0): {cat_dim0.shape}\n{cat_dim0}\n")
    
    cat_dim1 = torch.cat([t1, t2], dim=1)
    print(f"cat([t1, t2], dim=1): {cat_dim1.shape}\n{cat_dim1}\n")
    
    # STACK: Yeni boyut ekleyerek birleÅŸtir
    print("â”€"*70)
    print("ðŸ”¹ STACK - Yeni Boyut Ekleyerek BirleÅŸtir")
    
    stacked_dim0 = torch.stack([t1, t2], dim=0)
    print(f"stack([t1, t2], dim=0): {stacked_dim0.shape}\n{stacked_dim0}\n")
    
    stacked_dim1 = torch.stack([t1, t2], dim=1)
    print(f"stack([t1, t2], dim=1): {stacked_dim1.shape}\n{stacked_dim1}")
    
    print(f"\nðŸ’¡ cat vs stack:")
    print(f"  - cat: Mevcut boyutta birleÅŸtir")
    print(f"  - stack: Yeni boyut ekleyerek birleÅŸtir")


def demonstrate_common_pitfalls() -> None:
    """
    SÄ±k yapÄ±lan hatalarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 6: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    # HATA 1: view() ile boyut uyumsuzluÄŸu
    print("ðŸ”´ HATA 1: view() Boyut UyumsuzluÄŸu")
    
    tensor = torch.arange(12)
    
    try:
        # YANLIÅž: Toplam eleman sayÄ±sÄ± eÅŸleÅŸmiyor
        wrong_view = tensor.view(3, 5)  # 12 â‰  15
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"\nðŸ’¡ Ã‡Ã–ZÃœM: -1 kullan (otomatik hesaplama)")
        correct_view = tensor.view(3, -1)
        print(f"tensor.view(3, -1): {correct_view.shape}\n")
    
    # HATA 2: permute() sonrasÄ± view()
    print("â”€"*70)
    print("ðŸ”´ HATA 2: permute() SonrasÄ± view()")
    
    tensor = torch.randn(2, 3, 4)
    permuted = tensor.permute(2, 0, 1)
    
    try:
        # YANLIÅž: permute() non-contiguous yapar
        wrong = permuted.view(-1)
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"\nðŸ’¡ Ã‡Ã–ZÃœM: .contiguous() ekle")
        correct = permuted.contiguous().view(-1)
        print(f"permuted.contiguous().view(-1): {correct.shape}\n")
    
    # HATA 3: In-place iÅŸlem sonrasÄ± reshape
    print("â”€"*70)
    print("ðŸ”´ HATA 3: In-place Ä°ÅŸlem SonrasÄ± Reshape")
    
    tensor = torch.randn(3, 4, requires_grad=True)
    
    # YANLIÅž: In-place iÅŸlem gradient graph'Ä± bozar
    # tensor.add_(1.0)
    # reshaped = tensor.view(12)  # Gradient hatasÄ±!
    
    # DOÄžRU: Yeni tensor dÃ¶ndÃ¼r
    tensor_new = tensor.add(1.0)
    reshaped = tensor_new.view(12)
    print(f"âœ… DoÄŸru: tensor.add(1.0).view(12)")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ðŸš€ TENSOR MANÄ°PÃœLASYONU - VIEW, RESHAPE, PERMUTE".center(70))
    print("="*70)
    
    demonstrate_view_vs_reshape()
    demonstrate_permute_and_transpose()
    demonstrate_squeeze_and_unsqueeze()
    demonstrate_flatten_and_unflatten()
    demonstrate_advanced_manipulations()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 04 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
