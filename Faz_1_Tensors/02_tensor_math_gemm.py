"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 02: TENSOR MATEMATÄ°ÄžÄ° - GEMM VE BROADCASTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: GEMM (General Matrix Multiply) algoritmasÄ±nÄ± anlamak.
Broadcasting kurallarÄ±nÄ± Ã¶ÄŸrenmek ve Vectorization avantajlarÄ±nÄ± gÃ¶rmek.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
import time
from typing import Tuple, List
import matplotlib.pyplot as plt


def inspect_operation(name: str, tensor: torch.Tensor, operation: str = "") -> None:
    """
    Bir tensor iÅŸleminin detaylarÄ±nÄ± yazdÄ±rÄ±r.
    
    Args:
        name: Ä°ÅŸlem adÄ±
        tensor: SonuÃ§ tensor
        operation: Ä°ÅŸlem aÃ§Ä±klamasÄ±
    """
    print(f"\n{'â”€'*70}")
    print(f"ðŸ”¬ {name}")
    if operation:
        print(f"ðŸ“ Ä°ÅŸlem: {operation}")
    print(f"{'â”€'*70}")
    print(f"Shape: {tensor.shape}")
    print(f"Dtype: {tensor.dtype}")
    print(f"Device: {tensor.device}")
    print(f"Requires Grad: {tensor.requires_grad}")
    print(f"Data:\n{tensor}")
    print(f"{'â”€'*70}")


def demonstrate_matrix_multiplication_types() -> None:
    """
    FarklÄ± Ã§arpma tÃ¼rlerini (element-wise, dot, matmul) karÅŸÄ±laÅŸtÄ±rÄ±r.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 1: Ã‡ARPMA TÃœRLERÄ° - ELEMENT-WISE VS DOT VS MATMUL".center(70, "â”"))
    
    # 1D Tensor'lar
    a = torch.tensor([1, 2, 3], dtype=torch.float32)
    b = torch.tensor([4, 5, 6], dtype=torch.float32)
    
    print(f"\nðŸ“Š VektÃ¶rler:")
    print(f"a = {a}")
    print(f"b = {b}")
    
    # Element-wise Ã§arpma (Hadamard product)
    element_wise = a * b
    inspect_operation(
        "Element-wise Ã‡arpma (a * b)", 
        element_wise,
        "[1*4, 2*5, 3*6] = [4, 10, 18]"
    )
    
    # Dot product (Ä°Ã§ Ã§arpÄ±m)
    dot = torch.dot(a, b)
    inspect_operation(
        "Dot Product (torch.dot)", 
        dot,
        "1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32"
    )
    
    # @ operatÃ¶rÃ¼ (matmul iÃ§in)
    dot_operator = a @ b
    inspect_operation(
        "@ OperatÃ¶rÃ¼ (a @ b)", 
        dot_operator,
        "1D tensor'larda dot product ile aynÄ±"
    )
    
    # 2D Matrisler
    print("\n" + "â”€"*70)
    print("ðŸ“Š Matrisler:")
    A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
    B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)
    
    print(f"\nA (2x2):\n{A}")
    print(f"\nB (2x2):\n{B}")
    
    # Element-wise Ã§arpma
    element_wise_2d = A * B
    inspect_operation(
        "Element-wise Ã‡arpma (A * B)", 
        element_wise_2d,
        "Her eleman kendi karÅŸÄ±lÄ±ÄŸÄ±yla Ã§arpÄ±lÄ±r"
    )
    
    # Matrix multiplication (GEMM)
    matmul = A @ B
    inspect_operation(
        "Matrix Multiplication (A @ B)", 
        matmul,
        "C[i,j] = Î£(A[i,k] * B[k,j])"
    )
    
    # Manuel hesaplama doÄŸrulamasÄ±
    print("\nðŸ” MANUEL DOÄžRULAMA:")
    print(f"C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] = {A[0,0]}*{B[0,0]} + {A[0,1]}*{B[1,0]} = {A[0,0]*B[0,0] + A[0,1]*B[1,0]}")
    print(f"C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1] = {A[0,0]}*{B[0,1]} + {A[0,1]}*{B[1,1]} = {A[0,0]*B[0,1] + A[0,1]*B[1,1]}")
    print(f"SonuÃ§ matrisi:\n{matmul}")


def demonstrate_gemm_performance() -> None:
    """
    GEMM optimizasyonlarÄ±nÄ± ve performans farklarÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 2: GEMM PERFORMANSI - NAIVE VS OPTIMIZED".center(70, "â”"))
    
    # Naive implementasyon (3 nested loop)
    def naive_matmul(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """
        ÃœÃ§ iÃ§ iÃ§e dÃ¶ngÃ¼ ile matris Ã§arpÄ±mÄ± (EÄŸitim amaÃ§lÄ±).
        ASLA production'da kullanma!
        """
        m, k = A.shape
        k2, n = B.shape
        assert k == k2, "Ä°Ã§ boyutlar eÅŸleÅŸmeli!"
        
        C = torch.zeros(m, n, dtype=A.dtype)
        
        # O(m * n * k) karmaÅŸÄ±klÄ±k
        for i in range(m):
            for j in range(n):
                for p in range(k):
                    C[i, j] += A[i, p] * B[p, j]
        
        return C
    
    # Test matrisleri
    size = 128
    A = torch.randn(size, size)
    B = torch.randn(size, size)
    
    print(f"\nðŸ“Š Test Matrisleri: {size}x{size}")
    
    # Naive implementasyon
    start = time.time()
    C_naive = naive_matmul(A, B)
    naive_time = time.time() - start
    print(f"\nâ±ï¸  Naive (3 Loop): {naive_time:.4f} saniye")
    
    # PyTorch optimized GEMM
    start = time.time()
    C_optimized = A @ B
    optimized_time = time.time() - start
    print(f"â±ï¸  PyTorch GEMM: {optimized_time:.6f} saniye")
    
    # HÄ±z farkÄ±
    speedup = naive_time / optimized_time
    print(f"\nðŸš€ HIZ ARTIÅžI: {speedup:.0f}x daha hÄ±zlÄ±!")
    print(f"ðŸ’¡ Sebep: BLAS/cuBLAS kÃ¼tÃ¼phaneleri (C++/CUDA optimizasyonu)")
    
    # SonuÃ§ doÄŸrulamasÄ±
    diff = torch.abs(C_naive - C_optimized).max()
    print(f"\nâœ… SonuÃ§ DoÄŸrulamasÄ±: Max fark = {diff:.2e} (Neredeyse sÄ±fÄ±r)")


def demonstrate_broadcasting_rules() -> None:
    """
    PyTorch broadcasting kurallarÄ±nÄ± detaylÄ± aÃ§Ä±klar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 3: BROADCASTING - OTOMATÄ°K BOYUT GENÄ°ÅžLETME".center(70, "â”"))
    
    print("\nðŸ“œ BROADCASTING KURALLARI:")
    print("1. SaÄŸdan sola doÄŸru boyutlarÄ± karÅŸÄ±laÅŸtÄ±r")
    print("2. Ä°ki boyut eÅŸit VEYA birisi 1 ise uyumlu")
    print("3. Eksik boyutlar 1 kabul edilir")
    print("4. Uyumsuz boyutlar hata verir\n")
    
    # Ã–rnek 1: Skaler ile tensor
    print("â”€"*70)
    print("ðŸ“Š Ã–RNEK 1: Skaler + Tensor")
    tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
    scalar = 10.0
    
    result = tensor + scalar
    inspect_operation(
        "Skaler Broadcasting",
        result,
        f"{tensor.shape} + () â†’ {scalar} tÃ¼m elemanlara eklenir"
    )
    
    # Ã–rnek 2: 1D + 2D
    print("\n" + "â”€"*70)
    print("ðŸ“Š Ã–RNEK 2: 1D Tensor + 2D Tensor")
    matrix = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)  # (2, 3)
    vector = torch.tensor([10, 20, 30], dtype=torch.float32)  # (3,)
    
    result = matrix + vector
    print(f"\nMatrix shape: {matrix.shape}")
    print(f"Vector shape: {vector.shape}")
    print(f"Result shape: {result.shape}")
    print(f"\nMatrix:\n{matrix}")
    print(f"\nVector: {vector}")
    print(f"\nResult (her satÄ±ra vector eklendi):\n{result}")
    
    # Ã–rnek 3: FarklÄ± boyutlar
    print("\n" + "â”€"*70)
    print("ðŸ“Š Ã–RNEK 3: KarmaÅŸÄ±k Broadcasting")
    a = torch.randn(3, 1, 5)  # (3, 1, 5)
    b = torch.randn(1, 4, 5)  # (1, 4, 5)
    
    result = a + b
    print(f"\na shape: {a.shape}")
    print(f"b shape: {b.shape}")
    print(f"Result shape: {result.shape}")
    print(f"\nðŸ’¡ AÃ§Ä±klama:")
    print(f"  Dim 0: 3 vs 1 â†’ 3 (b geniÅŸler)")
    print(f"  Dim 1: 1 vs 4 â†’ 4 (a geniÅŸler)")
    print(f"  Dim 2: 5 vs 5 â†’ 5 (eÅŸit)")
    print(f"  SonuÃ§: (3, 4, 5)")
    
    # HATA Ã–RNEÄžÄ°
    print("\n" + "â”€"*70)
    print("ðŸ”´ HATA Ã–RNEÄžÄ°: Uyumsuz Boyutlar")
    x = torch.randn(3, 4)
    y = torch.randn(5, 4)
    
    try:
        result = x + y
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"\nðŸ’¡ Sebep:")
        print(f"  x: (3, 4)")
        print(f"  y: (5, 4)")
        print(f"  Dim 0: 3 vs 5 â†’ Uyumsuz! (Ä°kisi de 1 deÄŸil)")


def demonstrate_vectorization_advantage() -> None:
    """
    Vectorization'Ä±n performans avantajÄ±nÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 4: VECTORIZATION - DÃ–NGÃœSÃœZ HESAPLAMA".center(70, "â”"))
    
    # Problem: 1 milyon elemanlÄ± iki vektÃ¶rÃ¼ topla
    size = 1_000_000
    a = torch.randn(size)
    b = torch.randn(size)
    
    print(f"\nðŸ“Š Problem: {size:,} elemanlÄ± vektÃ¶r toplama")
    
    # YÃ¶ntem 1: Python loop (KÃ–TÃœ)
    start = time.time()
    result_loop = torch.zeros(size)
    for i in range(size):
        result_loop[i] = a[i] + b[i]
    loop_time = time.time() - start
    print(f"\nâ±ï¸  Python Loop: {loop_time:.4f} saniye")
    
    # YÃ¶ntem 2: Vectorized (Ä°YÄ°)
    start = time.time()
    result_vec = a + b
    vec_time = time.time() - start
    print(f"â±ï¸  Vectorized: {vec_time:.6f} saniye")
    
    # HÄ±z farkÄ±
    speedup = loop_time / vec_time
    print(f"\nðŸš€ HIZ ARTIÅžI: {speedup:.0f}x daha hÄ±zlÄ±!")
    
    print(f"\nðŸ’¡ Sebep:")
    print(f"  - Vectorized iÅŸlemler CPU SIMD (Single Instruction Multiple Data) kullanÄ±r")
    print(f"  - Bir komutla 4-8-16 eleman aynÄ± anda iÅŸlenir")
    print(f"  - Python loop'ta her iterasyon iÃ§in interpreter overhead var")
    
    # Batch iÅŸlemler
    print("\n" + "â”€"*70)
    print("ðŸ“Š BATCH Ä°ÅžLEMLER (Broadcasting + Vectorization)")
    
    # 1000 vektÃ¶rÃ¼n her birine farklÄ± skaler ekle
    vectors = torch.randn(1000, 512)  # (batch, features)
    scalars = torch.randn(1000, 1)    # (batch, 1)
    
    start = time.time()
    result = vectors + scalars  # Broadcasting!
    batch_time = time.time() - start
    
    print(f"\nVectors: {vectors.shape}")
    print(f"Scalars: {scalars.shape}")
    print(f"Result: {result.shape}")
    print(f"â±ï¸  SÃ¼re: {batch_time:.6f} saniye")
    print(f"ðŸ’¡ 1000 iÅŸlem tek seferde yapÄ±ldÄ± (SIMD + Broadcasting)")


def demonstrate_common_pitfalls() -> None:
    """
    SÄ±k yapÄ±lan hatalarÄ± ve Ã§Ã¶zÃ¼mlerini gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 5: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    # HATA 1: YanlÄ±ÅŸ boyut sÄ±rasÄ±
    print("\nðŸ”´ HATA 1: Matmul Boyut UyumsuzluÄŸu")
    A = torch.randn(3, 4)
    B = torch.randn(3, 5)
    
    print(f"A: {A.shape}, B: {B.shape}")
    
    try:
        # YANLIÅž: Ä°Ã§ boyutlar eÅŸleÅŸmiyor
        C = A @ B
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
        print(f"\nðŸ’¡ Ã‡Ã–ZÃœM: B'yi transpose et")
        B_correct = torch.randn(4, 5)
        C = A @ B_correct
        print(f"âœ… A @ B_correct: {A.shape} @ {B_correct.shape} = {C.shape}")
    
    # HATA 2: In-place iÅŸlem broadcasting'de
    print("\n" + "â”€"*70)
    print("ðŸ”´ HATA 2: In-place Broadcasting HatasÄ±")
    x = torch.randn(3, 4)
    y = torch.randn(4)
    
    try:
        # YANLIÅž: In-place iÅŸlem boyut deÄŸiÅŸtiremez
        x += y  # Bu Ã§alÄ±ÅŸÄ±r Ã§Ã¼nkÃ¼ result shape = (3, 4)
        print(f"âœ… x += y Ã§alÄ±ÅŸtÄ±: {x.shape}")
        
        # Ama tersi Ã§alÄ±ÅŸmaz
        y_test = torch.randn(4)
        x_test = torch.randn(3, 4)
        # y_test += x_test  # Bu hata verir!
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
    
    # HATA 3: Dtype uyumsuzluÄŸu
    print("\n" + "â”€"*70)
    print("ðŸ”´ HATA 3: Dtype KarÄ±ÅŸÄ±klÄ±ÄŸÄ±")
    int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)
    float_tensor = torch.tensor([1.5, 2.5, 3.5], dtype=torch.float32)
    
    # PyTorch otomatik type promotion yapar
    result = int_tensor + float_tensor
    print(f"int32 + float32 = {result.dtype}")
    print(f"ðŸ’¡ PyTorch otomatik olarak float32'ye yÃ¼kseltti")
    
    # Ama matmul'da dikkatli ol
    A_int = torch.randint(0, 10, (3, 4), dtype=torch.int32)
    B_int = torch.randint(0, 10, (4, 5), dtype=torch.int32)
    C_int = A_int @ B_int
    print(f"\nint32 @ int32 = {C_int.dtype}")
    print(f"âš ï¸  Overflow riski var! BÃ¼yÃ¼k deÄŸerlerde float kullan")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ðŸš€ TENSOR MATEMATÄ°ÄžÄ° - GEMM VE BROADCASTING".center(70))
    print("="*70)
    
    demonstrate_matrix_multiplication_types()
    demonstrate_gemm_performance()
    demonstrate_broadcasting_rules()
    demonstrate_vectorization_advantage()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 02 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
