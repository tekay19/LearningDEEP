"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DERS 03: GELÄ°ÅžMÄ°Åž Ä°NDEXLEME - MASKING, FANCY INDEXING VE VIEW VS COPY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

AmaÃ§: PyTorch'un geliÅŸmiÅŸ indexleme tekniklerini Ã¶ÄŸrenmek.
Boolean masking, fancy indexing ve view/copy ayrÄ±mÄ±nÄ± anlamak.

Hedef Kitle: Senior Developer'lar iÃ§in "Under the Hood" analiz.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import torch
import numpy as np
from typing import Tuple, List, Optional
import time


def demonstrate_basic_indexing() -> None:
    """
    Temel indexleme tÃ¼rlerini gÃ¶sterir: integer, slice, ellipsis.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 1: TEMEL Ä°NDEXLEME - INTEGER, SLICE, ELLIPSIS".center(70, "â”"))
    
    tensor = torch.arange(24).reshape(2, 3, 4)
    print(f"ðŸ“Š Orijinal Tensor (2Ã—3Ã—4):\n{tensor}")
    print(f"Shape: {tensor.shape}\n")
    
    # Integer indexing
    print("â”€"*70)
    print("ðŸ”¹ INTEGER INDEXING")
    element = tensor[0, 1, 2]
    print(f"tensor[0, 1, 2] = {element}")
    print(f"Shape: {element.shape} (0D tensor - scalar)")
    print(f"Storage paylaÅŸÄ±mÄ±: {tensor.data_ptr() == element.data_ptr()}")
    
    # Slice indexing
    print("\n" + "â”€"*70)
    print("ðŸ”¹ SLICE INDEXING")
    sliced = tensor[0, :, 1:3]
    print(f"tensor[0, :, 1:3]:\n{sliced}")
    print(f"Shape: {sliced.shape}")
    print(f"Stride: {sliced.stride()}")
    print(f"Is contiguous: {sliced.is_contiguous()}")
    print(f"Storage paylaÅŸÄ±mÄ±: {tensor.data_ptr() == sliced.data_ptr()} (VIEW!)")
    
    # Ellipsis (...) kullanÄ±mÄ±
    print("\n" + "â”€"*70)
    print("ðŸ”¹ ELLIPSIS (...) KULLANIMI")
    print(f"tensor[..., 0] (Son boyutta 0. index):\n{tensor[..., 0]}")
    print(f"tensor[0, ...] (Ä°lk boyutta 0. index):\n{tensor[0, ...]}")
    print(f"\nðŸ’¡ Ellipsis = 'Geri kalan tÃ¼m boyutlar'")


def demonstrate_boolean_masking() -> None:
    """
    Boolean masking ile koÅŸullu indexleme gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 2: BOOLEAN MASKING - KOÅžULLU Ä°NDEXLEME".center(70, "â”"))
    
    data = torch.tensor([1, -2, 3, -4, 5, -6, 7, -8], dtype=torch.float32)
    print(f"ðŸ“Š Veri: {data}\n")
    
    # Boolean mask oluÅŸturma
    mask = data > 0
    print(f"ðŸŽ­ Mask (data > 0): {mask}")
    print(f"Mask dtype: {mask.dtype}")
    print(f"Mask shape: {mask.shape}\n")
    
    # Masking ile filtreleme
    positive_values = data[mask]
    print(f"âœ… Pozitif deÄŸerler: {positive_values}")
    print(f"Shape: {positive_values.shape}")
    
    # âš ï¸ KRÄ°TÄ°K: Boolean indexing COPY oluÅŸturur!
    print(f"\nðŸ”´ UYARI: Boolean indexing COPY oluÅŸturur!")
    print(f"Orijinal data pointer: {data.data_ptr()}")
    print(f"Filtered data pointer: {positive_values.data_ptr()}")
    print(f"AynÄ± mÄ±? {data.data_ptr() == positive_values.data_ptr()} (HAYIR!)\n")
    
    # torch.where kullanÄ±mÄ±
    print("â”€"*70)
    print("ðŸ”¹ torch.where() - KOÅžULLU DEÄžER ATAMA")
    
    # Negatif deÄŸerleri 0 yap
    result = torch.where(data > 0, data, torch.tensor(0.0))
    print(f"torch.where(data > 0, data, 0):")
    print(f"Orijinal: {data}")
    print(f"SonuÃ§:    {result}")
    print(f"\nðŸ’¡ where(condition, x, y) â†’ condition True ise x, False ise y")
    
    # Ã‡ok boyutlu masking
    print("\n" + "â”€"*70)
    print("ðŸ”¹ Ã‡OK BOYUTLU MASKING")
    
    matrix = torch.randn(4, 5)
    print(f"Matris:\n{matrix}")
    
    # 0'dan bÃ¼yÃ¼k elemanlarÄ± bul
    mask_2d = matrix > 0
    print(f"\nMask (matrix > 0):\n{mask_2d}")
    
    positive_elements = matrix[mask_2d]
    print(f"\nPozitif elemanlar (1D): {positive_elements}")
    print(f"Shape: {positive_elements.shape} (DÃ¼zleÅŸtirildi!)")
    
    # In-place masking
    print("\n" + "â”€"*70)
    print("ðŸ”¹ IN-PLACE MASKING")
    
    matrix_copy = matrix.clone()
    matrix_copy[matrix_copy < 0] = 0  # Negatif deÄŸerleri sÄ±fÄ±rla
    print(f"Negatifler sÄ±fÄ±rlandÄ±:\n{matrix_copy}")


def demonstrate_fancy_indexing() -> None:
    """
    Fancy indexing (tensor indexing) tekniklerini gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 3: FANCY INDEXING - TENSOR Ä°LE Ä°NDEXLEME".center(70, "â”"))
    
    data = torch.arange(10, 20)  # [10, 11, 12, ..., 19]
    print(f"ðŸ“Š Veri: {data}\n")
    
    # Integer tensor ile indexleme
    indices = torch.tensor([0, 2, 5, 7])
    selected = data[indices]
    print(f"ðŸ”¹ INTEGER TENSOR INDEXING")
    print(f"Indices: {indices}")
    print(f"data[indices]: {selected}")
    print(f"\nâš ï¸  Bu da COPY oluÅŸturur!")
    print(f"AynÄ± storage? {data.data_ptr() == selected.data_ptr()} (HAYIR!)\n")
    
    # 2D fancy indexing
    print("â”€"*70)
    print("ðŸ”¹ 2D FANCY INDEXING")
    
    matrix = torch.arange(20).reshape(4, 5)
    print(f"Matris (4Ã—5):\n{matrix}\n")
    
    # Belirli satÄ±rlarÄ± seÃ§
    row_indices = torch.tensor([0, 2, 3])
    selected_rows = matrix[row_indices]
    print(f"SatÄ±r indices: {row_indices}")
    print(f"SeÃ§ilen satÄ±rlar:\n{selected_rows}\n")
    
    # SatÄ±r VE sÃ¼tun indexleme
    row_idx = torch.tensor([0, 1, 2, 3])
    col_idx = torch.tensor([0, 2, 4, 1])
    
    diagonal_elements = matrix[row_idx, col_idx]
    print(f"SatÄ±r indices: {row_idx}")
    print(f"SÃ¼tun indices: {col_idx}")
    print(f"SeÃ§ilen elemanlar: {diagonal_elements}")
    print(f"ðŸ’¡ matrix[i, j] â†’ [matrix[0,0], matrix[1,2], matrix[2,4], matrix[3,1]]")
    
    # Advanced: Broadcasting ile fancy indexing
    print("\n" + "â”€"*70)
    print("ðŸ”¹ BROADCASTING + FANCY INDEXING")
    
    # Her satÄ±rdan farklÄ± sÃ¼tunlarÄ± seÃ§
    row_idx = torch.arange(4).unsqueeze(1)  # (4, 1)
    col_idx = torch.tensor([[0, 2], [1, 3], [2, 4], [0, 1]])  # (4, 2)
    
    result = matrix[row_idx, col_idx]
    print(f"Row indices (4Ã—1):\n{row_idx}")
    print(f"Col indices (4Ã—2):\n{col_idx}")
    print(f"SonuÃ§ (4Ã—2):\n{result}")


def demonstrate_view_vs_copy() -> None:
    """
    View ve Copy arasÄ±ndaki kritik farklarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 4: VIEW VS COPY - BELLEK PAYLAÅžIMI".center(70, "â”"))
    
    original = torch.arange(12, dtype=torch.float32).reshape(3, 4)
    print(f"ðŸ“Š Orijinal Tensor:\n{original}\n")
    
    # VIEW oluÅŸturan iÅŸlemler
    print("â”€"*70)
    print("âœ… VIEW OLUÅžTURAN Ä°ÅžLEMLER (Storage paylaÅŸÄ±mÄ±)")
    
    operations = [
        ("Slice", original[1:3]),
        ("Transpose", original.t()),
        ("View", original.view(4, 3)),
        ("Reshape (contiguous)", original.reshape(2, 6)),
        ("Narrow", original.narrow(0, 0, 2)),
        ("Expand", original[:, :2].expand(3, 4)),
    ]
    
    for name, tensor in operations:
        is_same_storage = original.data_ptr() == tensor.data_ptr()
        print(f"{name:20} â†’ Storage paylaÅŸÄ±mÄ±: {is_same_storage}")
    
    # COPY oluÅŸturan iÅŸlemler
    print("\n" + "â”€"*70)
    print("âŒ COPY OLUÅžTURAN Ä°ÅžLEMLER (Yeni storage)")
    
    copy_operations = [
        ("Clone", original.clone()),
        ("Boolean Indexing", original[original > 5]),
        ("Fancy Indexing", original[torch.tensor([0, 2])]),
        ("Contiguous", original.t().contiguous()),
        ("Detach + Clone", original.detach().clone()),
    ]
    
    for name, tensor in copy_operations:
        is_same_storage = original.data_ptr() == tensor.data_ptr()
        print(f"{name:20} â†’ Storage paylaÅŸÄ±mÄ±: {is_same_storage}")
    
    # View'da deÄŸiÅŸiklik yapma
    print("\n" + "â”€"*70)
    print("ðŸ”´ VIEW'DA DEÄžÄ°ÅžÄ°KLÄ°K YAPMA TESTÄ°")
    
    view_tensor = original[0, :]  # Ä°lk satÄ±r (view)
    print(f"View (ilk satÄ±r): {view_tensor}")
    
    view_tensor[0] = 999
    print(f"View deÄŸiÅŸtirildi â†’ view_tensor[0] = 999")
    print(f"Orijinal tensor:\n{original}")
    print(f"ðŸ’¡ Orijinal de deÄŸiÅŸti! (AynÄ± storage)")
    
    # Copy'de deÄŸiÅŸiklik yapma
    print("\n" + "â”€"*70)
    print("âœ… COPY'DE DEÄžÄ°ÅžÄ°KLÄ°K YAPMA TESTÄ°")
    
    original = torch.arange(12, dtype=torch.float32).reshape(3, 4)  # Reset
    copy_tensor = original.clone()
    
    copy_tensor[0, 0] = 777
    print(f"Copy deÄŸiÅŸtirildi â†’ copy_tensor[0,0] = 777")
    print(f"Orijinal tensor:\n{original}")
    print(f"ðŸ’¡ Orijinal deÄŸiÅŸmedi! (FarklÄ± storage)")


def demonstrate_advanced_techniques() -> None:
    """
    GeliÅŸmiÅŸ indexleme teknikleri ve optimizasyonlar.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 5: GELÄ°ÅžMÄ°Åž TEKNÄ°KLER VE OPTÄ°MÄ°ZASYONLAR".center(70, "â”"))
    
    # torch.masked_select
    print("ðŸ”¹ torch.masked_select() - MASKING Ä°LE SEÃ‡ME")
    
    data = torch.randn(3, 4)
    mask = data > 0
    
    selected = torch.masked_select(data, mask)
    print(f"Data:\n{data}")
    print(f"Mask:\n{mask}")
    print(f"SeÃ§ilen elemanlar: {selected}")
    print(f"Shape: {selected.shape} (1D!)\n")
    
    # torch.masked_fill
    print("â”€"*70)
    print("ðŸ”¹ torch.masked_fill() - MASKING Ä°LE DOLDURMA")
    
    data_copy = data.clone()
    data_copy.masked_fill_(mask, 0.0)
    print(f"Pozitif deÄŸerler 0 yapÄ±ldÄ±:\n{data_copy}\n")
    
    # torch.index_select
    print("â”€"*70)
    print("ðŸ”¹ torch.index_select() - BOYUT BAZLI SEÃ‡ME")
    
    matrix = torch.arange(20).reshape(4, 5)
    indices = torch.tensor([0, 2, 3])
    
    selected = torch.index_select(matrix, dim=0, index=indices)
    print(f"Matris:\n{matrix}")
    print(f"Dim 0'da indices {indices} seÃ§ildi:\n{selected}\n")
    
    # torch.gather
    print("â”€"*70)
    print("ðŸ”¹ torch.gather() - GELÄ°ÅžMÄ°Åž TOPLAMA")
    
    scores = torch.tensor([
        [0.1, 0.3, 0.6],
        [0.4, 0.2, 0.4],
        [0.7, 0.1, 0.2]
    ])
    
    # Her satÄ±rdan en yÃ¼ksek skorun indexini bul
    max_indices = scores.argmax(dim=1, keepdim=True)
    print(f"Scores:\n{scores}")
    print(f"Max indices (dim=1):\n{max_indices}")
    
    # gather ile en yÃ¼ksek skorlarÄ± al
    max_scores = torch.gather(scores, dim=1, index=max_indices)
    print(f"Max scores:\n{max_scores}\n")
    
    # Performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("â”€"*70)
    print("ðŸ”¹ PERFORMANS KARÅžILAÅžTIRMASI")
    
    big_tensor = torch.randn(10000, 1000)
    mask = big_tensor > 0
    
    # YÃ¶ntem 1: Boolean indexing
    start = time.time()
    result1 = big_tensor[mask]
    time1 = time.time() - start
    
    # YÃ¶ntem 2: masked_select
    start = time.time()
    result2 = torch.masked_select(big_tensor, mask)
    time2 = time.time() - start
    
    # YÃ¶ntem 3: where + flatten
    start = time.time()
    result3 = torch.where(mask, big_tensor, torch.tensor(float('nan'))).flatten()
    result3 = result3[~torch.isnan(result3)]
    time3 = time.time() - start
    
    print(f"Boolean indexing:  {time1:.6f}s")
    print(f"masked_select:     {time2:.6f}s")
    print(f"where + flatten:   {time3:.6f}s")
    print(f"\nðŸ’¡ En hÄ±zlÄ±: {'Boolean indexing' if time1 < min(time2, time3) else 'masked_select' if time2 < time3 else 'where + flatten'}")


def demonstrate_common_pitfalls() -> None:
    """
    SÄ±k yapÄ±lan hatalarÄ± gÃ¶sterir.
    """
    print("\n" + "ðŸŽ¯ BÃ–LÃœM 6: YAYGIN HATALAR VE Ã‡Ã–ZÃœMLER".center(70, "â”"))
    
    # HATA 1: View Ã¼zerinde in-place iÅŸlem
    print("ðŸ”´ HATA 1: View Ãœzerinde In-place Ä°ÅŸlem")
    
    original = torch.arange(12, dtype=torch.float32).reshape(3, 4)
    original.requires_grad = True
    
    view = original[0, :]
    
    try:
        # YANLIÅž: View Ã¼zerinde in-place iÅŸlem gradient graph'Ä± bozar
        # view.add_(1.0)  # Bu satÄ±r aÃ§Ä±lÄ±rsa backward() hatasÄ± verir
        print("âš ï¸  view.add_(1.0) gradient graph'Ä± bozar!")
        
        # DOÄžRU: Yeni tensor dÃ¶ndÃ¼r
        new_view = view.add(1.0)
        print(f"âœ… DoÄŸru: new_view = view.add(1.0)")
    except RuntimeError as e:
        print(f"âŒ HATA: {e}")
    
    # HATA 2: Boolean indexing ile assignment
    print("\n" + "â”€"*70)
    print("ðŸ”´ HATA 2: Boolean Indexing ile Assignment")
    
    data = torch.randn(5)
    mask = data > 0
    
    # YANLIÅž: Boolean indexing copy oluÅŸturur
    # data[mask] = 0  # Bu Ã§alÄ±ÅŸÄ±r ama dikkatli ol!
    
    # DOÄžRU: masked_fill_ kullan
    data.masked_fill_(mask, 0.0)
    print(f"âœ… DoÄŸru: data.masked_fill_(mask, 0.0)")
    
    # HATA 3: Fancy indexing ile gradient
    print("\n" + "â”€"*70)
    print("ðŸ”´ HATA 3: Fancy Indexing ile Gradient")
    
    embeddings = torch.randn(100, 50, requires_grad=True)
    indices = torch.tensor([0, 5, 10])
    
    selected = embeddings[indices]
    loss = selected.sum()
    loss.backward()
    
    print(f"Embeddings gradient shape: {embeddings.grad.shape}")
    print(f"Non-zero gradients: {(embeddings.grad != 0).sum().item()}")
    print(f"ðŸ’¡ Sadece seÃ§ilen satÄ±rlarda gradient var (sparse gradient)")


def main() -> None:
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu.
    """
    print("\n" + "="*70)
    print("ðŸš€ GELÄ°ÅžMÄ°Åž Ä°NDEXLEME - MASKING VE FANCY INDEXING".center(70))
    print("="*70)
    
    demonstrate_basic_indexing()
    demonstrate_boolean_masking()
    demonstrate_fancy_indexing()
    demonstrate_view_vs_copy()
    demonstrate_advanced_techniques()
    demonstrate_common_pitfalls()
    
    print("\n" + "="*70)
    print("âœ… DERS 03 TAMAMLANDI!".center(70))
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
