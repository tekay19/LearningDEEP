# ğŸ¬ DERS 02: TENSOR MATEMATÄ°ÄÄ° - GEMM VE BROADCASTING

---

## ğŸ“º BLOK 1: PRODÃœKSÄ°YON VE SENARYO (YouTuber Modu)

### ğŸ¯ Video BaÅŸlÄ±ÄŸÄ±
**"GEMM Nedir ve Neden Her Deep Learning Framework'Ã¼n Kalbi? | Broadcasting Deep Dive"**

### ğŸ£ The Hook (0:00-0:45)
> "Bilgisayar biliminde en optimize edilmiÅŸ algoritma hangisidir? Sorting? HayÄ±r. Search? HayÄ±r. GEMM - General Matrix Multiply! TÃ¼m derin Ã¶ÄŸrenme modellerinin %90'Ä± aslÄ±nda GEMM Ã§aÄŸrÄ±sÄ±dÄ±r. NVIDIA'nÄ±n milyarlarca dolarlÄ±k GPU'larÄ± sadece bu iÅŸlemi hÄ±zlandÄ±rmak iÃ§in tasarlanmÄ±ÅŸtÄ±r. BugÃ¼n PyTorch'un matris Ã§arpÄ±mÄ±nÄ± nasÄ±l 1000x hÄ±zlandÄ±rdÄ±ÄŸÄ±nÄ± ve broadcasting'in sihirli kurallarÄ±nÄ± Ã¶ÄŸreneceksiniz. Hadi baÅŸlayalÄ±m!"

### ğŸ¨ GÃ¶rselleÅŸtirme Ä°puÃ§larÄ±

1. **1:00-2:00**: GEMM Animasyonu
   - Ekrana iki matris (A: 3x4, B: 4x5) gÃ¶ster
   - C[0,0] hesaplanÄ±rken: A'nÄ±n 0. satÄ±rÄ± ile B'nin 0. sÃ¼tunu element-wise Ã§arpÄ±lÄ±p toplanÄ±rken animasyon gÃ¶ster
   - Her adÄ±mda Ã§arpÄ±lan elemanlar yanÄ±p sÃ¶nsÃ¼n
   - SonuÃ§: "1Ã—5 + 2Ã—9 + 3Ã—13 + 4Ã—17 = 110"

2. **4:30-5:30**: Naive vs Optimized KarÅŸÄ±laÅŸtÄ±rmasÄ±
   - Split screen: Solda 3 iÃ§ iÃ§e dÃ¶ngÃ¼ (yavaÅŸ), saÄŸda BLAS kÃ¼tÃ¼phanesi (hÄ±zlÄ±)
   - Solda dÃ¶ngÃ¼ler dÃ¶nÃ¼yor (yavaÅŸ animasyon), saÄŸda tek seferde "BOOM!" sonuÃ§ Ã§Ä±kÄ±yor
   - AltÄ±nda hÄ±z karÅŸÄ±laÅŸtÄ±rmasÄ±: "128x128 matris â†’ Naive: 2.5s, BLAS: 0.002s (1250x hÄ±zlÄ±!)"

3. **7:00-9:00**: Broadcasting KurallarÄ±
   - Ekrana iki tensor gÃ¶ster: (3, 1, 5) ve (1, 4, 5)
   - SaÄŸdan sola boyutlarÄ± karÅŸÄ±laÅŸtÄ±r (animasyonla)
   - Uyumlu boyutlar yeÅŸil, uyumsuz kÄ±rmÄ±zÄ± iÅŸaretle
   - SonuÃ§ tensor'u (3, 4, 5) ÅŸeklinde geniÅŸlerken gÃ¶ster

4. **11:00-12:00**: Vectorization GÃ¼cÃ¼
   - 1 milyon elemanlÄ± iki vektÃ¶r gÃ¶ster
   - Python loop: Her eleman tek tek toplanÄ±yor (yavaÅŸ)
   - SIMD: 8 eleman aynÄ± anda toplanÄ±yor (hÄ±zlÄ±)
   - CPU register'larÄ±nda paralel iÅŸlem animasyonu

---

## ğŸ§  BLOK 3: DERÄ°N TEORÄ°K ANALÄ°Z (Akademisyen Modu)

### ğŸ“ Matematiksel Temeller

#### 1. GEMM FormÃ¼lÃ¼
**General Matrix Multiply (GEMM):**

```
C = Î± Ã— (A @ B) + Î² Ã— C

Burada:
- A: (m Ã— k) matrisi
- B: (k Ã— n) matrisi
- C: (m Ã— n) matrisi
- Î±, Î²: Skaler katsayÄ±lar
```

**Element-wise aÃ§Ä±lÄ±m:**
```
C[i, j] = Î± Ã— Î£(k=0 to K-1) A[i, k] Ã— B[k, j] + Î² Ã— C[i, j]
```

**KarmaÅŸÄ±klÄ±k Analizi:**
- **Zaman:** O(m Ã— n Ã— k) â†’ 3 iÃ§ iÃ§e dÃ¶ngÃ¼
- **Bellek:** O(mÃ—k + kÃ—n + mÃ—n)
- **FLOP Count:** 2mnk (Her eleman iÃ§in k Ã§arpma + k toplama)

**Ã–rnek:** (1000 Ã— 1000) @ (1000 Ã— 1000)
- FLOP: 2 Ã— 1000Â³ = 2 milyar iÅŸlem
- Modern GPU (A100): ~312 TFLOPS â†’ ~6.4 mikrosaniye!

---

#### 2. Broadcasting KurallarÄ± (Formal TanÄ±m)

**Kural 1:** SaÄŸdan sola boyutlarÄ± karÅŸÄ±laÅŸtÄ±r
```python
A: (5, 3, 4, 1)
B:    (3, 1, 7)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Result: (5, 3, 4, 7)
```

**Kural 2:** Ä°ki boyut uyumludur âŸº (eÅŸit VEYA birisi 1)
```
Uyumlu:
  3 vs 3 âœ…
  3 vs 1 âœ…
  1 vs 7 âœ…

Uyumsuz:
  3 vs 5 âŒ (Ä°kisi de 1 deÄŸil ve eÅŸit deÄŸil)
```

**Kural 3:** Eksik boyutlar 1 kabul edilir
```python
A: (4, 5)    â†’  (1, 4, 5)  # Sol tarafa 1 eklenir
B: (5,)      â†’  (1, 1, 5)
```

**Matematiksel GÃ¶sterim:**
```
Broadcast(A, B) = C
where C[iâ‚, iâ‚‚, ..., iâ‚™] = A[jâ‚, jâ‚‚, ..., jâ‚˜] âŠ™ B[kâ‚, kâ‚‚, ..., kâ‚š]

jâ‚“ = iâ‚“ if A.shape[x] > 1 else 0
kâ‚“ = iâ‚“ if B.shape[x] > 1 else 0
```

---

### âš™ï¸ Under The Hood (Kaputun AltÄ±)

#### BLAS/cuBLAS KÃ¼tÃ¼phaneleri

**1. CPU: BLAS (Basic Linear Algebra Subprograms)**

PyTorch CPU'da matris Ã§arpÄ±mÄ± iÃ§in Intel MKL veya OpenBLAS kullanÄ±r:

```cpp
// PyTorch C++ backend
// aten/src/ATen/native/LinearAlgebra.cpp

Tensor matmul_cpu(const Tensor& A, const Tensor& B) {
  // Intel MKL'nin SGEMM fonksiyonunu Ã§aÄŸÄ±r
  cblas_sgemm(
    CblasRowMajor,    // Row-major order
    CblasNoTrans,     // A transpose edilmemiÅŸ
    CblasNoTrans,     // B transpose edilmemiÅŸ
    m, n, k,          // Boyutlar
    1.0,              // alpha
    A.data_ptr(),     // A pointer
    lda,              // Leading dimension
    B.data_ptr(),     // B pointer
    ldb,
    0.0,              // beta
    C.data_ptr(),     // C pointer
    ldc
  );
}
```

**Intel MKL OptimizasyonlarÄ±:**
- **Cache Blocking:** Matrisi kÃ¼Ã§Ã¼k bloklara bÃ¶l, L1/L2 cache'e sÄ±ÄŸdÄ±r
- **Loop Unrolling:** DÃ¶ngÃ¼ overhead'ini azalt
- **SIMD (AVX-512):** 16 float'u aynÄ± anda iÅŸle
- **Multi-threading:** OpenMP ile paralel hesaplama

---

**2. GPU: cuBLAS (CUDA BLAS)**

GPU'da NVIDIA'nÄ±n cuBLAS kÃ¼tÃ¼phanesi kullanÄ±lÄ±r:

```cpp
// PyTorch CUDA backend
// aten/src/ATen/native/cuda/Blas.cpp

Tensor matmul_cuda(const Tensor& A, const Tensor& B) {
  cublasHandle_t handle = getCurrentCUDABlasHandle();
  
  // cuBLAS SGEMM Ã§aÄŸrÄ±sÄ±
  cublasSgemm(
    handle,
    CUBLAS_OP_N,      // B transpose edilmemiÅŸ
    CUBLAS_OP_N,      // A transpose edilmemiÅŸ
    n, m, k,
    &alpha,
    B.data_ptr(),     // cuBLAS column-major kullanÄ±r!
    ldb,
    A.data_ptr(),
    lda,
    &beta,
    C.data_ptr(),
    ldc
  );
}
```

**cuBLAS OptimizasyonlarÄ±:**
- **Tensor Cores (A100):** 4Ã—4 matris bloklarÄ±nÄ± tek cycle'da Ã§arp
- **Warp-level Primitives:** 32 thread aynÄ± anda Ã§alÄ±ÅŸÄ±r
- **Shared Memory:** On-chip bellek (L1 cache benzeri)
- **Kernel Fusion:** Birden fazla iÅŸlemi tek kernel'da birleÅŸtir

---

#### GEMM Performans Analizi

**Roofline Model:**
```
Performans = min(Peak FLOPS, Bandwidth Ã— Arithmetic Intensity)

Arithmetic Intensity (AI) = FLOP / Byte
GEMM AI = 2mnk / (4(mk + kn + mn))  # float32 iÃ§in 4 byte

Ã–rnek: (1024 Ã— 1024) @ (1024 Ã— 1024)
AI = 2Ã—1024Â³ / (4Ã—3Ã—1024Â²) â‰ˆ 170 FLOP/Byte
â†’ Compute-bound (Bellek deÄŸil, hesaplama sÄ±nÄ±rlÄ±)
```

**GPU KullanÄ±m OranÄ±:**
```python
import torch

A = torch.randn(4096, 4096, device='cuda')
B = torch.randn(4096, 4096, device='cuda')

# Profiling
with torch.profiler.profile() as prof:
    C = A @ B

print(prof.key_averages().table())
# Ã‡Ä±ktÄ±: ~95% GPU kullanÄ±mÄ± (Ã‡ok iyi!)
```

---

### ğŸ­ SektÃ¶r Notu: Production OrtamÄ±nda KarÅŸÄ±laÅŸÄ±lan Sorunlar

#### Problem 1: Mixed Precision Training'de Overflow

**Senaryo:** FP16 (half precision) kullanÄ±rken matris Ã§arpÄ±mÄ±nda overflow.

```python
# YANLIÅ
A = torch.randn(1000, 1000, dtype=torch.float16, device='cuda')
B = torch.randn(1000, 1000, dtype=torch.float16, device='cuda')
C = A @ B  # Overflow riski! FP16 max: 65504
```

**Ã‡Ã¶zÃ¼m:** Automatic Mixed Precision (AMP)
```python
# DOÄRU
from torch.cuda.amp import autocast

with autocast():
    C = A @ B  # PyTorch otomatik FP32'ye yÃ¼kseltir
```

---

#### Problem 2: Broadcasting ile Beklenmedik Bellek KullanÄ±mÄ±

**Senaryo:** BÃ¼yÃ¼k tensor'lara broadcasting uygulanÄ±rken OOM (Out of Memory).

```python
# YANLIÅ
big_tensor = torch.randn(1000, 1000, 1000, device='cuda')  # 4 GB
small_tensor = torch.randn(1000, device='cuda')            # 4 KB

result = big_tensor + small_tensor  # small_tensor (1000, 1000, 1000)'e geniÅŸler!
# GeÃ§ici bellek: 4 GB ekstra â†’ OOM!
```

**Ã‡Ã¶zÃ¼m:** In-place iÅŸlem
```python
# DOÄRU
big_tensor.add_(small_tensor)  # In-place, ekstra bellek yok
```

---

#### Problem 3: Batch Matmul'da Boyut KarÄ±ÅŸÄ±klÄ±ÄŸÄ±

**Senaryo:** Transformer'da attention hesaplamasÄ±.

```python
# YANLIÅ
Q = torch.randn(32, 8, 128, 64)  # (batch, heads, seq_len, d_k)
K = torch.randn(32, 8, 128, 64)

# Hata: Son iki boyut uyumsuz (64 @ 64)
scores = Q @ K  # RuntimeError!
```

**Ã‡Ã¶zÃ¼m:** Transpose
```python
# DOÄRU
scores = Q @ K.transpose(-2, -1)  # (32, 8, 128, 64) @ (32, 8, 64, 128)
# SonuÃ§: (32, 8, 128, 128) âœ…
```

---

### ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ä°ÅŸlem | CPU (Intel i9) | GPU (RTX 3090) | GPU (A100) | HÄ±zlanma |
|-------|---------------|----------------|------------|----------|
| (1024Ã—1024) @ (1024Ã—1024) | 15 ms | 0.8 ms | 0.3 ms | 50x |
| (4096Ã—4096) @ (4096Ã—4096) | 980 ms | 12 ms | 4 ms | 245x |
| Batch (32, 512, 512) | 1200 ms | 18 ms | 6 ms | 200x |

**Not:** A100'Ã¼n Tensor Core'larÄ± FP16'da 312 TFLOPS ulaÅŸÄ±r!

---

### ğŸ”¬ Derin DalÄ±ÅŸ: Tensor Core Mimarisi

**NVIDIA Tensor Core (A100):**
```
Bir Tensor Core cycle'da ÅŸunu yapar:
D = A Ã— B + C

Burada:
- A: 4Ã—4 matris (FP16)
- B: 4Ã—4 matris (FP16)
- C: 4Ã—4 matris (FP32)
- D: 4Ã—4 matris (FP32)

Toplam: 64 FLOP (4Ã—4Ã—4 Ã§arpma + 4Ã—4 toplama) tek cycle'da!
```

**KullanÄ±m:**
```python
# PyTorch otomatik Tensor Core kullanÄ±r (FP16 + AMP)
with torch.cuda.amp.autocast():
    C = A @ B  # Tensor Core aktif!
```

---

## âš”ï¸ BLOK 4: MEYDAN OKUMA (Ã–dev)

### ğŸ¯ GÃ¶rev: Cache-Optimized GEMM Implementasyonu

**Zorluk Seviyesi:** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ (Ä°leri)

**AÃ§Ä±klama:**
Naive GEMM'den daha hÄ±zlÄ± bir implementasyon yazÄ±n. Cache blocking tekniÄŸini kullanarak Intel MKL'ye yakÄ±n performans elde edin.

**Gereksinimler:**

```python
import torch
import time

def blocked_matmul(A: torch.Tensor, B: torch.Tensor, block_size: int = 64) -> torch.Tensor:
    """
    Cache-aware matris Ã§arpÄ±mÄ±.
    
    Args:
        A: (m, k) tensor
        B: (k, n) tensor
        block_size: Cache'e sÄ±ÄŸacak blok boyutu
    
    Returns:
        C: (m, n) tensor
    """
    m, k = A.shape
    k2, n = B.shape
    assert k == k2
    
    C = torch.zeros(m, n, dtype=A.dtype)
    
    # TODO: Matrisleri block_size Ã— block_size bloklara bÃ¶l
    # TODO: Her blok Ã§iftini Ã§arp (cache'de kal)
    # TODO: SonuÃ§larÄ± C'ye akÃ¼mÃ¼le et
    
    return C

# Test
sizes = [128, 256, 512, 1024]
for size in sizes:
    A = torch.randn(size, size)
    B = torch.randn(size, size)
    
    # Naive
    start = time.time()
    C_naive = naive_matmul(A, B)  # Ders 02'deki fonksiyon
    naive_time = time.time() - start
    
    # Blocked
    start = time.time()
    C_blocked = blocked_matmul(A, B, block_size=64)
    blocked_time = time.time() - start
    
    # PyTorch
    start = time.time()
    C_torch = A @ B
    torch_time = time.time() - start
    
    print(f"\nSize: {size}Ã—{size}")
    print(f"Naive:   {naive_time:.4f}s")
    print(f"Blocked: {blocked_time:.4f}s (Speedup: {naive_time/blocked_time:.1f}x)")
    print(f"PyTorch: {torch_time:.6f}s")
    
    # DoÄŸruluk kontrolÃ¼
    assert torch.allclose(C_blocked, C_torch, atol=1e-4)
```

**Bonus GÃ¶revler:**
1. **Loop Unrolling:** Ä°Ã§ dÃ¶ngÃ¼yÃ¼ 4'lÃ¼ gruplar halinde aÃ§
2. **SIMD Simulation:** `torch.sum()` yerine manuel toplama yap
3. **Profiling:** Hangi blok boyutu en hÄ±zlÄ±? (16, 32, 64, 128 dene)
4. **Visualization:** Blok boyutuna gÃ¶re performans grafiÄŸi Ã§iz

**Beklenen SonuÃ§:**
- Naive'den en az 5-10x hÄ±zlÄ±
- PyTorch'tan yavaÅŸ ama yakÄ±n (2-5x fark kabul edilebilir)

---

### âœ… BaÅŸarÄ± Kriterleri
1. âœ… Cache blocking doÄŸru uygulandÄ± mÄ±?
2. âœ… Naive implementasyondan en az 5x hÄ±zlÄ± mÄ±?
3. âœ… SonuÃ§lar PyTorch ile eÅŸleÅŸiyor mu? (atol=1e-4)
4. âœ… FarklÄ± blok boyutlarÄ±nÄ± test ettiniz mi?

---

## ğŸ“š Ek Kaynaklar

- [BLAS (Basic Linear Algebra Subprograms)](http://www.netlib.org/blas/)
- [Intel MKL Documentation](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html)
- [NVIDIA cuBLAS](https://docs.nvidia.com/cuda/cublas/)
- [Anatomy of High-Performance Matrix Multiplication](https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf)
- [PyTorch Broadcasting Semantics](https://pytorch.org/docs/stable/notes/broadcasting.html)

---

**ğŸ¬ Sonraki Ders:** `03_indexing_advanced.py` - Masking, Fancy Indexing ve View vs Copy
